{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "#R_LOSS_FACTOR = 10000\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/'\n",
    "new_model = load_model(\"/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/val_acc_9280_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6e-07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=K.eval(new_model.optimizer.lr)\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataframe=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')\n",
    "cengage= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/cengage_production_data.csv')\n",
    "pearson= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/pearson_production_data.csv')\n",
    "wiley=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/wiley_production_data.csv')\n",
    "wiley.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "pearson.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "cengage.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, cengage, on='Name', how='left')\n",
    "image_dataframe.drop(['image_path', 'alt_text_html', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, pearson, on='Name', how='left')\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text_x\"].astype(str) + \" \"+image_dataframe[\"alt_text_y\"].astype(str)\n",
    "image_dataframe['subject_area'] = image_dataframe['subject_area_x'].astype(str) + \" \"+ image_dataframe['subject_area_y'].astype(str)\n",
    "image_dataframe.drop(['alt_text_x', \"alt_text_y\", 'subject_area_x', 'subject_area_y', 'alt_text_html', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, wiley, on='Name', how='left')\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text_x\"].astype(str) + \" \"+ image_dataframe[\"alt_text_y\"].astype(str)\n",
    "image_dataframe['subject_area'] = image_dataframe['subject_area_x'].astype(str) + \" \"+ image_dataframe['subject_area_y'].astype(str)\n",
    "image_dataframe.drop(['alt_text_x', \"alt_text_y\", 'subject_area_x', 'subject_area_y', 'alt_text_html', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "banned = ['nan']\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])\n",
    "image_dataframe[\"subject_area\"] = image_dataframe[\"subject_area\"].apply(f)\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text\"].apply(f)\n",
    "image_dataframe = image_dataframe[image_dataframe.subject_area != '']\n",
    "image_dataframe.subject_area=image_dataframe.subject_area.replace({\"other other\": \"others\", \"other\": \"others\"})\n",
    "image_dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "include = ['maths', 'science', 'others', 'emss', 'canada', 'engineering', 'business', 'history']\n",
    "indexNames=[]\n",
    "for i in range(image_dataframe.shape[0]):\n",
    "    if(image_dataframe.loc[i]['subject_area'] not in include):\n",
    "        indexNames.append(i)\n",
    "image_dataframe.drop(labels=indexNames, inplace=True)\n",
    "image_dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maths          146839\n",
       "science         10820\n",
       "others           5116\n",
       "emss             3602\n",
       "canada           2852\n",
       "engineering      1854\n",
       "business         1681\n",
       "history          1014\n",
       "Name: subject_area, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe['subject_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['maths', 'history', 'science', 'engineering', 'business', 'others',\n",
       "       'emss', 'canada'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.subject_area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name            0\n",
       "Path            0\n",
       "Height          0\n",
       "Width           0\n",
       "alt_text        0\n",
       "subject_area    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>subject_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781337111348_11348_ch02_2.1_017-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>181</td>\n",
       "      <td>The image consists of a Minitab output. Visual...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781285194790_9781285194790-551-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>550</td>\n",
       "      <td>593</td>\n",
       "      <td>A map shows the Second Iraq War. Majority popu...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781133947257_9781133947257_ch01_65-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>101</td>\n",
       "      <td>184</td>\n",
       "      <td>In the Sherlock Holmes mystery The Final Solut...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781305652231_52231_ch01_f022-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>24</td>\n",
       "      <td>188</td>\n",
       "      <td>The image consists of a number line. The numbe...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ceng_image205.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>268</td>\n",
       "      <td>The image consists of a visual representation ...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name  \\\n",
       "0     9781337111348_11348_ch02_2.1_017-t2.png   \n",
       "1      9781285194790_9781285194790-551-t2.png   \n",
       "2  9781133947257_9781133947257_ch01_65-t2.png   \n",
       "3        9781305652231_52231_ch01_f022-t2.png   \n",
       "4                           ceng_image205.png   \n",
       "\n",
       "                                                Path  Height  Width  \\\n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     131    181   \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     550    593   \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     101    184   \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...      24    188   \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     131    268   \n",
       "\n",
       "                                            alt_text subject_area  \n",
       "0  The image consists of a Minitab output. Visual...        maths  \n",
       "1  A map shows the Second Iraq War. Majority popu...      history  \n",
       "2  In the Sherlock Holmes mystery The Final Solut...        maths  \n",
       "3  The image consists of a number line. The numbe...        maths  \n",
       "4  The image consists of a visual representation ...        maths  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173778 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "def gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = np.expand_dims(gray_image, axis=-1)\n",
    "    return gray_image\n",
    "BATCH_SIZE=32\n",
    "trdata = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "traindata = trdata.flow_from_dataframe(dataframe = image_dataframe\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = False\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5431"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(round(image_dataframe.shape[0]/BATCH_SIZE ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430.5625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.shape[0]/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_gen(dataframe):\n",
    "   \n",
    "    final_model = Model(inputs=new_model.inputs, outputs=new_model.layers[-2].output)\n",
    "    # summarize\n",
    "    print(final_model.summary())\n",
    "   \n",
    "    features = []\n",
    "    for i in tqdm(range(int(round(dataframe.shape[0]/BATCH_SIZE ,0)))):\n",
    "    \n",
    "        images, labels = next(traindata)\n",
    "        # get features\n",
    "        feature = final_model.predict(images, verbose=0)\n",
    "        \n",
    "        features.extend(feature)\n",
    "        \n",
    "    features=np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%|          | 0/5431 [00:00<?, ?it/s]Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 119,545,856\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n",
      "100%|██████████| 5431/5431 [27:30<00:00,  3.29it/s]\n",
      "Extracted Features: 173778\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "features = extract_features_gen(image_dataframe)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/'\n",
    "# save to file\n",
    "dump(features, open(RUN_FOLDER+'features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 4096)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "\t\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent =sent.replace(' x ', ' x-axis ')\n",
    "    sent=sent.replace(' y ', ' y-axis ')\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    sent = ' '.join(w for w in word_tokens if not w in stop_words and len(w)>2)\n",
    "    sent= re.sub(r\"(\\d+\\s?)+\",\" number \",sent)\n",
    "\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173778/173778 [06:40<00:00, 433.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(image_dataframe.shape[0])):\n",
    "    clean=clean_sentence(image_dataframe.loc[i]['alt_text'])\n",
    "    #print(clean)\n",
    "    image_dataframe['alt_text'][i]='startseq ' + clean + ' endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq four curves graphed x-axis y-axis coordinate plane horizontal axis labeled x-axis vertical axis labeled y-axis curves labeled y-axis x-axis x number c number value different curve curve enters bottom third curve goes right enters second quadrant intersects y-axis axis enters first quadrant curve enters bottom third curve goes right enters second quadrant intersects y-axis axis enters first quadrant curve enters bottom third curve goes right passes origin enters first quadrant goes right negative curve enters bottom third curve goes right intersects negative y-axis axis negative goes right enters first quadrant curve negative curve curve curve endseq'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_dataframe['alt_text'][0]=clean\n",
    "image_dataframe['alt_text'][12456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataframe.to_csv('cleantext_alt_text.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>subject_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781337111348_11348_ch02_2.1_017-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>181</td>\n",
       "      <td>startseq image consists minitab output visual ...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781285194790_9781285194790-551-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>550</td>\n",
       "      <td>593</td>\n",
       "      <td>startseq map shows second iraq war majority po...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781133947257_9781133947257_ch01_65-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>101</td>\n",
       "      <td>184</td>\n",
       "      <td>startseq sherlock holmes mystery final solutio...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781305652231_52231_ch01_f022-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>24</td>\n",
       "      <td>188</td>\n",
       "      <td>startseq image consists number line numbers nu...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ceng_image205.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>268</td>\n",
       "      <td>startseq image consists visual representation ...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name  \\\n",
       "0     9781337111348_11348_ch02_2.1_017-t2.png   \n",
       "1      9781285194790_9781285194790-551-t2.png   \n",
       "2  9781133947257_9781133947257_ch01_65-t2.png   \n",
       "3        9781305652231_52231_ch01_f022-t2.png   \n",
       "4                           ceng_image205.png   \n",
       "\n",
       "                                                Path  Height  Width  \\\n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     131    181   \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     550    593   \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     101    184   \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...      24    188   \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     131    268   \n",
       "\n",
       "                                            alt_text subject_area  \n",
       "0  startseq image consists minitab output visual ...        maths  \n",
       "1  startseq map shows second iraq war majority po...      history  \n",
       "2  startseq sherlock holmes mystery final solutio...        maths  \n",
       "3  startseq image consists number line numbers nu...        maths  \n",
       "4  startseq image consists visual representation ...        maths  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= pd.read_csv('cleantext_alt_text.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "def vocab(sent):\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    for i in word_tokens:\n",
    "        try:\n",
    "            dict[i]+=1\n",
    "        except:\n",
    "            dict[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173778/173778 [01:30<00:00, 1918.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test.shape[0])):\n",
    "    vocab(test.loc[i]['alt_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74489"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary= set(dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74489"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= test.loc[:,:]\n",
    "y= test.loc[:,'subject_area']\n",
    "X_train=X[0:130333]\n",
    "X_test=X[130333:]\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130333, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43445, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130333/130333 [00:28<00:00, 4565.71it/s]\n"
     ]
    }
   ],
   "source": [
    "all_alt_text = list()\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    all_alt_text.append(X_train.loc[i]['alt_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43445/43445 [00:09<00:00, 4610.37it/s]\n"
     ]
    }
   ],
   "source": [
    "test_all_alt_text = list()\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    test_all_alt_text.append(X_test.loc[i]['alt_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130333 43445\n"
     ]
    }
   ],
   "source": [
    "print(len(all_alt_text), len(test_all_alt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52413"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_alt_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='features.pkl'\n",
    "infile = open(filename,'rb')\n",
    "photo_features = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 4096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return self.it.__next__()\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threadsafe_generator\n",
    "def data_generator(batch_size,tokenizer, vocab_size, photo_features, all_alt_text):\n",
    "    global generator_index\n",
    "    print(\"Generator Initiated\", generator_index)\n",
    "    generator_index+=1\n",
    "    while True:\t\t\n",
    "        num_samples = len(all_alt_text)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            photo = photo_features[offset : offset + batch_size]\n",
    "            alt_text = all_alt_text[offset : offset + batch_size]\n",
    "            print(\" \",offset, \" I am offset of Generator index \",generator_index)\n",
    "            \n",
    "            X1, X2, y = list(), list(), list()\n",
    "            for j in range(len(alt_text)):\n",
    "                seq = tokenizer.texts_to_sequences([alt_text[j]])[0]\n",
    "                \n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq =seq[:i], seq[i]\n",
    "                    \n",
    "                    in_seq = pad_sequences([in_seq], maxlen=200)[0]   #200 = max_length\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    X1.append(photo[j])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "                    \n",
    "                    \n",
    "            #in_img, in_seq, out_word = create_sequences(tokenizer, vocab_size, photo, alt_text)\n",
    "            yield [array(X1), array(X2)], array(y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class Data_Gen(Sequence):\n",
    "    '''Generates Data for Keras'''\n",
    "    def __init__(self, batch_size, tokenizer, vocab_size, photo_features, alt_text):\n",
    "        '''Initialization'''\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.photo_features = photo_features\n",
    "        self.alt_text = alt_text\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):           #Must be implemented for every Sequence Item\n",
    "        '''Denotes the no of batches per epoch'''\n",
    "        return int(np.floor(self.photo_features.shape[0]/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):       #Must be implemented for every Sequence Item and it should return a complete Batch\n",
    "        '''Generate one batch of data'''\n",
    "        # Generate indexes of the batch\n",
    "        #indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]   #indexes is not present in the Main Sequence Keras Class\n",
    "        photo = self.photo_features[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        alt_text = self.alt_text[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        X1, X2, y = list(), list(), list()\n",
    "        for j in range(len(alt_text)):\n",
    "            seq = tokenizer.texts_to_sequences([alt_text[j]])[0]\n",
    "\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq =seq[:i], seq[i]\n",
    "\n",
    "                in_seq = pad_sequences([in_seq], maxlen=200)[0]   #200 = max_length\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1.append(photo[j])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "\n",
    "        #in_img, in_seq, out_word = create_sequences(tokenizer, vocab_size, photo, alt_text)\n",
    "        return [array(X1), array(X2)], array(y)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "\n",
    "    \n",
    "    #def on_epoch_end(self):                           Optional function can be used to do shuffling\n",
    "     #   'Updates indexes after each epoch'\n",
    "      #  self.indexes = np.arange(len(self.list_IDs))\n",
    "       # if self.shuffle == True:\n",
    "        #    np.random.shuffle(self.indexes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130333, 4096) (43445, 4096)\n"
     ]
    }
   ],
   "source": [
    "train_photo_features=photo_features[0:130333]\n",
    "test_photo_features=photo_features[130333:]\n",
    "print(train_photo_features.shape, test_photo_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4072"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(int(np.floor(train_photo_features.shape[0]/32)))\n",
    "np.arange(len(self.list_IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "global generator_index\n",
    "generator_index =-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Data_Gen' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-34530c4e3b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_Gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_photo_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_alt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Data_Gen' object is not an iterator"
     ]
    }
   ],
   "source": [
    "batch_size=3\n",
    "generator = Data_Gen(batch_size, tokenizer, vocab_size, train_photo_features, all_alt_text)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 4096)\n",
      "(89, 200)\n",
      "(89, 52413)\n"
     ]
    }
   ],
   "source": [
    "#global test_generator_index\n",
    "#test_generator_index=0\n",
    "batch_size=3\n",
    "#test_generator = data_generator(batch_size, tokenizer, vocab_size, test_photo_features, test_all_alt_text)\n",
    "test_inputs, test_outputs = next(test_generator)\n",
    "print(test_inputs[0].shape)\n",
    "print(test_inputs[1].shape)\n",
    "print(test_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "108\n",
      "120\n",
      "150\n",
      "283\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "#print(len(tokenizer.texts_to_sequences([all_alt_text[0]])[0])-1)\n",
    "#print(len(tokenizer.texts_to_sequences([all_alt_text[0]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[1]])[0])-2)\n",
    "print(len(tokenizer.texts_to_sequences([all_alt_text[0]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[1]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[2]])[0]) -3)\n",
    "print(len(tokenizer.texts_to_sequences([all_alt_text[3]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[4]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[5]])[0]) -3)\n",
    "print(len(tokenizer.texts_to_sequences([all_alt_text[6]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[7]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[8]])[0]) -3)\n",
    "print(len(tokenizer.texts_to_sequences([all_alt_text[9]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[10]])[0])+len(tokenizer.texts_to_sequences([all_alt_text[11]])[0]) -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "56\n",
      "74\n",
      "104\n",
      "80\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "#print(len(tokenizer.texts_to_sequences([test_all_alt_text[0]])[0])-1)\n",
    "#print(len(tokenizer.texts_to_sequences([test_all_alt_text[0]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[1]])[0])-2)\n",
    "print(len(tokenizer.texts_to_sequences([test_all_alt_text[0]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[1]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[2]])[0]) -3)\n",
    "print(len(tokenizer.texts_to_sequences([test_all_alt_text[3]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[4]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[5]])[0]) -3)\n",
    "print(len(tokenizer.texts_to_sequences([test_all_alt_text[6]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[7]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[8]])[0]) -3)\n",
    "print(len(tokenizer.texts_to_sequences([test_all_alt_text[9]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[10]])[0])+len(tokenizer.texts_to_sequences([test_all_alt_text[11]])[0]) -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(25, activation='relu')(fe1)       #256\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(25)(se2)                           #256\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(25, activation='relu')(decoder1)           #256\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 256)     13417728    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200, 256)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 25)           102425      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 25)           28200       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 25)           0           dense_6[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 25)           650         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 52413)        1362738     dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,911,741\n",
      "Trainable params: 14,911,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "max_length=200\n",
    "vocab_size= 52413\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 52413 200 (130333, 4096) 300\n"
     ]
    }
   ],
   "source": [
    "print(batch_size, vocab_size, max_length, train_photo_features.shape, len(all_alt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN, TensorBoard, EarlyStopping\n",
    "import os\n",
    "from datetime import datetime\n",
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/'\n",
    "logdir = os.path.join(RUN_FOLDER, \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "#checkpoint_filepath=os.path.join(run_folder, \"model.h5\")\n",
    "checkpoint1 = ModelCheckpoint(os.path.join(RUN_FOLDER, \"caption_model.h5\"), monitor='loss',  save_best_only=True, verbose=1, mode= 'min')\n",
    "checkpoint2 = ModelCheckpoint(os.path.join(RUN_FOLDER, 'caption_weights.h5'), save_weights_only = True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, mode='min', verbose=1)\n",
    "EarlyStop = EarlyStopping(monitor='loss', min_delta=0.001, patience=31, verbose=1, mode='min', restore_best_weights=True)\n",
    "terminate = TerminateOnNaN()\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint1, checkpoint2, reduce_lr, tensorboard_callback, EarlyStop, terminate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130333\n",
      "43445\n"
     ]
    }
   ],
   "source": [
    "NUM_IMAGES = train_photo_features.shape[0]\n",
    "print(NUM_IMAGES)\n",
    "TEST_NUM_IMAGES = test_photo_features.shape[0]\n",
    "print(TEST_NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 89 samples\n",
      "96/96 - 5s - loss: 10.7925 - val_loss: 10.3645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb808080a58>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([inputs[0], inputs[1]], outputs, epochs=1, verbose=2, validation_data=([test_inputs[0], test_inputs[1]], test_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "train_photo_features=train_photo_features[0:300]\n",
    "NUM_IMAGES = train_photo_features.shape[0]\n",
    "#print(NUM_IMAGES)\n",
    "all_alt_text= all_alt_text[0:300]\n",
    "#print(len(all_alt_text))\n",
    "generator = data_generator(batch_size, tokenizer, vocab_size, train_photo_features, all_alt_text)\n",
    "\n",
    "test_photo_features=test_photo_features[0:300]\n",
    "TEST_NUM_IMAGES = test_photo_features.shape[0]\n",
    "#print(TEST_NUM_IMAGES)\n",
    "test_all_alt_text=test_all_alt_text[0:300]\n",
    "#print(len(test_all_alt_text))\n",
    "batch_size=16\n",
    "test_generator = data_generator(batch_size, tokenizer, vocab_size, test_photo_features, test_all_alt_text)\n",
    "\n",
    "global generator_index\n",
    "generator_index =-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16  I am offset of Generator index  0\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "Train for 18.75 steps\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/data_utils.py\", line 970, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/data_utils.py\", line 962, in pool_fn\n",
      "    initargs=(seqs, self.random_seed, get_worker_id_queue()))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 174, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "Process Keras_worker_ForkPoolWorker-19:\n",
      "Process Keras_worker_ForkPoolWorker-18:\n",
      "Process Keras_worker_ForkPoolWorker-21:\n",
      "Process Keras_worker_ForkPoolWorker-20:\n",
      "Process Keras_worker_ForkPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = generator\n",
    "    , shuffle = False\n",
    "    , epochs = 100\n",
    "    , initial_epoch = 0\n",
    "    , callbacks = callbacks_list\n",
    "    , steps_per_epoch = NUM_IMAGES / batch_size\n",
    "    , workers = 1\n",
    "    , use_multiprocessing = True\n",
    "    #, validation_data= test_generator\n",
    "    #, validation_steps= TEST_NUM_IMAGES / batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Initiated -1\n",
      "  0  I am offset of Generator index  0\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "Train for 18.75 steps\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "  16  I am offset of Generator index  0\n",
      "  16  I am offset of Generator index  0\n",
      "  16  I am offset of Generator index  0\n",
      "  32  I am offset of Generator index  0\n",
      "  32  I am offset of Generator index  0\n",
      "  32  I am offset of Generator index  0\n",
      "  48  I am offset of Generator index  0\n",
      "  48  I am offset of Generator index  0\n",
      "  48  I am offset of Generator index  0\n",
      " 1/18 [>.............................] - ETA: 2:57 - loss: 10.8665  64  I am offset of Generator index  0\n",
      "  64  I am offset of Generator index  0\n",
      "  64  I am offset of Generator index  0\n",
      " 2/18 [==>...........................] - ETA: 1:40 - loss: 10.8552  80  I am offset of Generator index  0\n",
      " 3/18 [===>..........................] - ETA: 1:10 - loss: 10.8383  80  I am offset of Generator index  0\n",
      " 4/18 [=====>........................] - ETA: 53s - loss: 10.8238   80  I am offset of Generator index  0\n",
      " 5/18 [=======>......................] - ETA: 42s - loss: 10.7976  96  I am offset of Generator index  0\n",
      " 6/18 [========>.....................] - ETA: 34s - loss: 10.7601  96  I am offset of Generator index  0\n",
      " 7/18 [==========>...................] - ETA: 28s - loss: 10.7199  96  I am offset of Generator index  0\n",
      " 8/18 [===========>..................] - ETA: 23s - loss: 10.6658  112  I am offset of Generator index  0\n",
      " 9/18 [=============>................] - ETA: 19s - loss: 10.5968  112  I am offset of Generator index  0\n",
      "10/18 [===============>..............] - ETA: 16s - loss: 10.4829  112  I am offset of Generator index  0\n",
      "11/18 [================>.............] - ETA: 13s - loss: 10.3535  128  I am offset of Generator index  0\n",
      "12/18 [==================>...........] - ETA: 11s - loss: 10.2065  128  I am offset of Generator index  0\n",
      "  128  I am offset of Generator index  0\n",
      "13/18 [===================>..........] - ETA: 9s - loss: 10.1898   144  I am offset of Generator index  0\n",
      "  144  I am offset of Generator index  0\n",
      "14/18 [=====================>........] - ETA: 7s - loss: 10.1566  144  I am offset of Generator index  0\n",
      "  160  I am offset of Generator index  0\n",
      "15/18 [=======================>......] - ETA: 6s - loss: 10.1076  160  I am offset of Generator index  0\n",
      "16/18 [========================>.....] - ETA: 4s - loss: 9.9379   160  I am offset of Generator index  0\n",
      "17/18 [==========================>...] - ETA: 2s - loss: 9.7634  176  I am offset of Generator index  0\n",
      "18/18 [===========================>..] - ETA: 1s - loss: 9.5869  176  I am offset of Generator index  0\n",
      "  176  I am offset of Generator index  0\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.45942, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/caption_model.h5\n",
      "  192  I am offset of Generator index  0\n",
      "  192  I am offset of Generator index  0\n",
      "  192  I am offset of Generator index  0\n",
      "\n",
      "Epoch 00001: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/caption_weights.h5\n",
      "  208  I am offset of Generator index  0\n",
      "  208  I am offset of Generator index  0\n",
      "19/18 [==============================] - 30s 2s/step - loss: 9.4451\n",
      "  208  I am offset of Generator index  0\n",
      "Epoch 2/100\n",
      " 1/18 [>.............................] - ETA: 11s - loss: 6.6395  224  I am offset of Generator index  0\n",
      " 2/18 [==>...........................] - ETA: 12s - loss: 6.5442  224  I am offset of Generator index  0\n",
      " 3/18 [===>..........................] - ETA: 12s - loss: 6.5388  224  I am offset of Generator index  0\n",
      "  240  I am offset of Generator index  0\n",
      " 4/18 [=====>........................] - ETA: 13s - loss: 6.5261  240  I am offset of Generator index  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-3:\n",
      "Process Keras_worker_ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "Process Keras_worker_ForkPoolWorker-2:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 341, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/18 [=======>......................] - ETA: 12s - loss: 6.5261\n",
      "Epoch 00002: loss improved from 9.45942 to 6.52323, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/caption_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 463, in _handle_results\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 251, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "_pickle.UnpicklingError: invalid load key, '\\xbb'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/caption_weights.h5\n",
      " 5/18 [=======>......................] - ETA: 17s - loss: 6.5261"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0c8d73ebe734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_IMAGES\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#, validation_data= test_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#, validation_steps= TEST_NUM_IMAGES / batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-4:\n",
      "Process Keras_worker_ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Process Keras_worker_ForkPoolWorker-5:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = generator\n",
    "    , shuffle = False\n",
    "    , epochs = 100\n",
    "    , initial_epoch = 0\n",
    "    , callbacks = callbacks_list\n",
    "    , steps_per_epoch = NUM_IMAGES / batch_size\n",
    "    , workers = 3\n",
    "    , use_multiprocessing = False\n",
    "    #, validation_data= test_generator\n",
    "    #, validation_steps= TEST_NUM_IMAGES / batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}