{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596382505365",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decipher the Information in URLs\n",
    "\n",
    "A lot of information can be encoded in a URL. Your web scraping journey will be much easier if you first become familiar with how URLs work and what they’re made of. Try to pick apart the URL of the site you’re currently on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.monster.com/jobs/search/?q=Software-Developer&where=Australia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can deconstruct the above URL into two main parts:\n",
    "\n",
    "    The base URL represents the path to the search functionality of the website. In the example above, the base URL is https://www.monster.com/jobs/search/.\n",
    "    The query parameters represent additional values that can be declared on the page. In the example above, the query parameters are ?q=Software-Developer&where=Australia.\n",
    "\n",
    "Any job you’ll search for on this website will use the same base URL. However, the query parameters will change depending on what you’re looking for. You can think of them as query strings that get sent to the database to retrieve specific records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query parameters generally consist of three things:\n",
    "\n",
    "    Start: The beginning of the query parameters is denoted by a question mark (?).\n",
    "    Information: The pieces of information constituting one query parameter are encoded in key-value pairs, where related keys and values are joined together by an equals sign (key=value).\n",
    "    Separator: Every URL can have multiple query parameters, which are separated from each other by an ampersand (&).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with this information, you can pick apart the URL’s query parameters into two key-value pairs:\n",
    "\n",
    "    q=Software-Developer selects the type of job you’re looking for.\n",
    "    where=Australia selects the location you’re looking for.\n",
    "\n",
    "Try to change the search parameters and observe how that affects your URL. Go ahead and enter new values in the search bar up top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "URL = 'https://www.monster.com/jobs/search/?q=Software-Developer&where=Australia'\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs an HTTP request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object.\n",
    "\n",
    "If you take a look at the downloaded content, then you’ll notice that it looks very similar to the HTML you were inspecting earlier with developer tools. To improve the structure of how the HTML is displayed in your console output, you can print the object’s .content attribute with pprint()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Websites\n",
    "\n",
    "The website you’re scraping in this tutorial serves static HTML content. In this scenario, the server that hosts the site sends back HTML documents that already contain all the data you’ll get to see as a user.\n",
    "\n",
    "When you inspected the page with developer tools earlier on, you discovered that a job posting consists of the following long and messy-looking HTML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be difficult to wrap your head around such a long block of HTML code. To make it easier to read, you can use an HTML formatter to automatically clean it up a little more. Good readability helps you better understand the structure of any code block. While it may or may not help to improve the formatting of the HTML, it’s always worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://webformatter.com/html -- html formatter link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML above definitely has a few confusing parts in it. For example, you can scroll to the right to see the large number of attributes that the <a> element has. Luckily, the class names on the elements that you’re interested in are relatively straightforward:\n",
    "\n",
    "    class=\"title\": the title of the job posting\n",
    "    class=\"company\": the company that offers the position\n",
    "    class=\"location\": the location where you’d be working\n",
    "\n",
    "In case you ever get lost in a large pile of HTML, remember that you can always go back to your browser and use developer tools to further explore the HTML structure interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Websites\n",
    "\n",
    "Some pages contain information that’s hidden behind a login. That means you’ll need an account to be able to see (and scrape) anything from the page. The process to make an HTTP request from your Python script is different than how you access a page from your browser. That means that just because you can log in to the page through your browser, that doesn’t mean you’ll be able to scrape it with your Python script.\n",
    "\n",
    "However, there are some advanced techniques that you can use with the requests to access the content behind logins. These techniques will allow you to log in to websites while making the HTTP request from within your script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Websites\n",
    "\n",
    "Static sites are easier to work with because the server sends you an HTML page that already contains all the information as a response. You can parse an HTML response with Beautiful Soup and begin to pick out the relevant data.\n",
    "\n",
    "On the other hand, with a dynamic website the server might not send back any HTML at all. Instead, you’ll receive JavaScript code as a response. This will look completely different from what you saw when you inspected the page with your browser’s developer tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To offload work from the server to the clients’ machines, many modern websites avoid crunching numbers on their servers whenever possible. Instead, they’ll send JavaScript code that your browser will execute locally to produce the desired HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, what happens in the browser is not related to what happens in your script. Your browser will diligently execute the JavaScript code it receives back from a server and create the DOM and HTML for you locally. However, doing a request to a dynamic website in your Python script will not provide you with the HTML page content.\n",
    "\n",
    "When you use requests, you’ll only receive what the server sends back. In the case of a dynamic website, you’ll end up with some JavaScript code, which you won’t be able to parse using Beautiful Soup. The only way to go from the JavaScript code to the content you’re interested in is to execute the code, just like your browser does. The requests library can’t do that for you, but there are other solutions that can.\n",
    "\n",
    "For example, requests-html is a project created by the author of the requests library that allows you to easily render JavaScript using syntax that’s similar to the syntax in requests. It also includes capabilities for parsing the data by using Beautiful Soup under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Another popular choice for scraping dynamic content is Selenium. You can think of Selenium as a slimmed-down browser that executes the JavaScript code for you before passing on the rendered HTML response to your script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Parse HTML Code With Beautiful Soup\n",
    "\n",
    "You’ve successfully scraped some HTML from the Internet, but when you look at it now, it just seems like a huge mess. There are tons of HTML elements here and there, thousands of attributes scattered around—and wasn’t there some JavaScript mixed in as well? It’s time to parse this lengthy code response with Beautiful Soup to make it more accessible and pick out the data that you’re interested in.\n",
    "\n",
    "Beautiful Soup is a Python library for parsing structured data. It allows you to interact with HTML in a similar way to how you would interact with a web page using developer tools. Beautiful Soup exposes a couple of intuitive functions you can use to explore the HTML you received. To get started, use your terminal to install the Beautiful Soup library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Elements by ID\n",
    "\n",
    "In an HTML web page, every element can have an id attribute assigned. As the name already suggests, that id attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID.\n",
    "\n",
    "Switch back to developer tools and identify the HTML object that contains all of the job postings. Explore by hovering over parts of the page and using right-click to Inspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Keep in mind that it’s helpful to periodically switch back to your browser and interactively explore the page using developer tools. This helps you learn how to find the exact elements you’re looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this writing, the element you’re looking for is a <div> with an id attribute that has the value \"ResultsContainer\". It has a couple of other attributes as well, but below is the gist of what you’re looking for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup allows you to find that specific element easily by its ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(id='ResultsContainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier viewing, you can .prettify() any Beautiful Soup object when you print it out. If you call this method on the results variable that you just assigned above, then you should see all the HTML contained within the <div>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use the element’s ID, you’re able to pick one element out from among the rest of the HTML. This allows you to work with only this specific part of the page’s HTML. It looks like the soup just got a little thinner! However, it’s still quite dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Elements by HTML Class Name\n",
    "\n",
    "You’ve seen that every job posting is wrapped in a <section> element with the class card-content. Now you can work with your new Beautiful Soup object called results and select only the job postings. These are, after all, the parts of the HTML that you’re interested in! You can do this in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elems = results.find_all('section', class_='card-content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you call .find_all() on a Beautiful Soup object, which returns an iterable containing all the HTML for all the job listings displayed on that page.\n",
    "\n",
    "Take a look at all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_elem in job_elems:\n",
    "    print(job_elem, end='\\n'*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s already pretty neat, but there’s still a lot of HTML! You’ve seen earlier that your page has descriptive class names on some elements. Let’s pick out only those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_elem in job_elems:\n",
    "    # Each job_elem is a new BeautifulSoup object.\n",
    "    # You can use the same methods on it as you did before.\n",
    "    title_elem = job_elem.find('h2', class_='title')\n",
    "    company_elem = job_elem.find('div', class_='company')\n",
    "    location_elem = job_elem.find('div', class_='location')\n",
    "    print(title_elem)\n",
    "    print(company_elem)\n",
    "    print(location_elem)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You’re getting closer and closer to the data you’re actually interested in. Still, there’s a lot going on with all those HTML tags and attributes floating around:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll see how to narrow down this output in the next section.\n",
    "Extract Text From HTML Elements\n",
    "\n",
    "For now, you only want to see the title, company, and location of each job posting. And behold! Beautiful Soup has got you covered. You can add .text to a Beautiful Soup object to return only the text content of the HTML elements that the object contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_elem in job_elems:\n",
    "    title_elem = job_elem.find('h2', class_='title')\n",
    "    company_elem = job_elem.find('div', class_='company')\n",
    "    location_elem = job_elem.find('div', class_='location')\n",
    "    print(title_elem.text)\n",
    "    print(company_elem.text)\n",
    "    print(location_elem.text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code snippet and you’ll see the text content displayed. However, you’ll also get a lot of whitespace. Since you’re now working with Python strings, you can .strip() the superfluous whitespace. You can also apply any other familiar Python string methods to further clean up your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The web is messy and you can’t rely on a page structure to be consistent throughout. Therefore, you’ll more often than not run into errors while parsing HTML.\n",
    "When you run the above code, you might encounter an AttributeError:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that’s the case, then take a step back and inspect your previous results. Were there any items with a value of None? You might have noticed that the structure of the page is not entirely uniform. There could be an advertisement in there that displays in a different way than the normal job postings, which may return different results. For this tutorial, you can safely disregard the problematic element and skip over it while parsing the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Senior/Lead Software Engineer, Browser\nMagic Leap, Inc.\nSunnyvale, CA; Plantation, FL (HQ); Austin, TX; Culver New York City, CA; Seattle, WA; Toronto, NY\n\nSQL BI (SSRS, SSIS) developer for Blackboard - NYC\nLanceSoft Inc\nNew york, WA\n\nAnalytics Developer\nConoco Phillips\nBrisbane, QLD\n\nPython Developer\nLanceSoft Inc\nWoodlands, WA\n\nAnalytics Developer\nConoco Phillips\nBrisbane, QLD\n\nSenior Software Engineer, Application Framework - Contractor\nMagic Leap, Inc.\nPlantation, FL (HQ); Toronto, ON; Sunnyvale, CA; Culver New York City, CA; Seattle, WA; Austin, TX\n\nAgile Engineer\nCommonwealth Superannuation Corporation (CSC)\nCanberra, ACT\n\nCustomer Solutions Architect (Software) Professional Services Cyber Security\nVarmour\nSydney, NSW\n\nSoftware Platform Architect\nMagic Leap, Inc.\nPlantation, FL; Sunnyvale, CA; Culver New York City, CA; Austin, TX; Seattle, WA; Toronto, NY\n\nJunior QA Analyst - Melbourne, Victoria\nMediaocean\nMelbourne, VIC\n\nSoftware Development Engineer\nAmazon Corporate LLC\nSeattle, WA\n\nPayroll Tester\nDialog Group\nBrisbane, QLD\n\nGrowth Strategic Account Executive\nTwilio\nSydney, New South Wales, NSW\n\nRunner\nVacasa\nThe Blue Mountains, ON\n\nTest Analyst\nDialog Group\nCanberra, ACT\n\nTechnical Program Manager, Professional Services, APAC\nTwilio\nSydney, New South Wales, NSW\n\nCustomer Experience Technical Analyst - Sydney, New South Wales\nMediaocean\nSydney, NSW\n\nTest Consultant\nDialog Group\nBrisbane, QLD\n\nSenior Test Analyst\nDialog Group\nCanberra, ACT\n\nBusiness Development Manager - New Business\nDialog Group\nNorth Sydney, NSW\n\nEnterprise Account Executive\nZuora\nSydney, NSW\n\nTechnical Writer\nRobert Half\nBoulder, WA\n\n"
    }
   ],
   "source": [
    "for job_elem in job_elems:\n",
    "    title_elem = job_elem.find('h2', class_='title')\n",
    "    company_elem = job_elem.find('div', class_='company')\n",
    "    location_elem = job_elem.find('div', class_='location')\n",
    "    if None in (title_elem, company_elem, location_elem):\n",
    "        continue\n",
    "    print(title_elem.text.strip())\n",
    "    print(company_elem.text.strip())\n",
    "    print(location_elem.text.strip())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore why one of the elements is returned as None. You can use the conditional statement you wrote above to print() out and inspect the relevant element in more detail. What do you think is going on there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Elements by Class Name and Text Content\n",
    "\n",
    "By now, you’ve cleaned up the list of jobs that you saw on the website. While that’s pretty neat already, you can make your script more useful. However, not all of the job listings seem to be developer jobs that you’d be interested in as a Python developer. So instead of printing out all of the jobs from the page, you’ll first filter them for some keywords.\n",
    "\n",
    "You know that job titles in the page are kept within <h2> elements. To filter only for specific ones, you can use the string argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all('h2', string='Python Developer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "python_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code finds all <h2> elements where the contained string matches 'Python Developer' exactly. Note that you’re directly calling the method on your first results variable. If you go ahead and print() the output of the above code snippet to your console, then you might be disappointed because it will probably be empty:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was definitely a job with that title in the search results, so why is it not showing up? When you use string= like you did above, your program looks for exactly that string. Any differences in capitalization or whitespace will prevent the element from matching. In the next section, you’ll find a way to make the string more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a Function to a Beautiful Soup Method\n",
    "\n",
    "In addition to strings, you can often pass functions as arguments to Beautiful Soup methods. You can change the previous line of code to use a function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all('h2',string=lambda text: 'python' in text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\n"
    }
   ],
   "source": [
    "print(len(python_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your program has found a match!\n",
    "\n",
    "Note: In case you still don’t get a match, try adapting your search string. The job offers on this page are constantly changing and there might not be a job listed that includes the substring 'python' in its title at the time that you’re working through this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Attributes From HTML Elements\n",
    "\n",
    "At this point, your Python script already scrapes the site and filters its HTML for relevant job postings. Well done! However, one thing that’s still missing is the link to apply for a job.\n",
    "\n",
    "While you were inspecting the page, you found that the link is part of the element that has the title HTML class. The current code strips away the entire link when accessing the .text attribute of its parent element. As you’ve seen before, .text only contains the visible text content of an HTML element. Tags and attributes are not part of that. To get the actual URL, you want to extract one of those attributes instead of discarding it.\n",
    "\n",
    "Look at the list of filtered results python_jobs that you created above. The URL is contained in the href attribute of the nested <a> tag. Start by fetching the <a> element. Then, extract the value of its href attribute using square-bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Python Developer\nApply here: https://job-openings.monster.com/python-developer-woodlands-wa-us-lancesoft-inc/4755ec59-d0db-4ce9-8385-b4df7c1e9f7c\n\n"
    }
   ],
   "source": [
    "python_jobs = results.find_all('h2',string=lambda text: \"python\" in text.lower())\n",
    "\n",
    "for p_job in python_jobs:\n",
    "    link = p_job.find('a')['href']\n",
    "    print(p_job.text.strip())\n",
    "    print(f\"Apply here: {link}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered results will only show links to job opportunities that include python in their title. You can use the same square-bracket notation to extract other HTML attributes as well. A common use case is to fetch the URL of a link, as you did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you’ve learned how to scrape data from the Web using Python, requests, and Beautiful Soup. You built a script that fetches job postings from the Internet and went through the full web scraping process from start to finish.\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "    Inspect the HTML structure of your target site with your browser’s developer tools\n",
    "    Gain insight into how to decipher the data encoded in URLs\n",
    "    Download the page’s HTML content using Python’s requests library\n",
    "    Parse the downloaded HTML with Beautiful Soup to extract relevant information\n",
    "\n",
    "With this general pipeline in mind and powerful libraries in your toolkit, you can go out and see what other websites you can scrape! Have fun, and remember to always be respectful and use your programming skills responsibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}