{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store large numpy array using tables, storing similarity matrix, Pandas merge, Matplotlib, Pandas rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sys  \n",
    "sys.path.insert(0, '/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/')\n",
    "from models.AE import Autoencoder\n",
    "from utils.loaders import load_model, ImageLabelLoader\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = (128,128,3)\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataframe=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = np.expand_dims(gray_image, axis=-1)\n",
    "    return gray_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181858 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "\n",
    "data_flow_dataframe = data_gen.flow_from_dataframe(dataframe = image_dataframe\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , target_size = INPUT_DIM[:2]\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = False\n",
    "                                         , class_mode = 'input'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-100epochs/\n"
     ]
    }
   ],
   "source": [
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-100epochs/'\n",
    "print(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = load_model(Autoencoder, RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "\n",
    "filename = RUN_FOLDER+ 'matrix.h5'\n",
    "ROW_SIZE = 181858\n",
    "NUM_COLUMNS = 200\n",
    "\n",
    "f = tables.open_file(filename, mode='w')\n",
    "atom = tables.Float64Atom()\n",
    "\n",
    "array_c = f.create_earray(f.root, 'data', atom, (0, NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/182 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 161/182 [04:43<00:54,  2.57s/it]/opt/conda/lib/python3.6/site-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "100%|██████████| 182/182 [07:09<00:00,  2.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(182)):\n",
    "    \n",
    "    example_batch = next(data_flow_dataframe)\n",
    "    example_images = example_batch[0]\n",
    "    z_points = ae.encoder.predict(example_images)\n",
    "    \n",
    "    for idx in range(z_points.shape[0]):\n",
    "        x = z_points[idx].reshape(1,200)\n",
    "        array_c.append(x)\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181858, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tables\n",
    "filename=RUN_FOLDER+'matrix.h5'\n",
    "h = tables.open_file(filename, mode='r')\n",
    "m=[]\n",
    "for i in range(181858):\n",
    "    m.append(h.root.data[i])\n",
    "m=np.array(m)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166418</td>\n",
       "      <td>9780134838076_unfig17-10.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>335</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64504</td>\n",
       "      <td>0321613376_p101_002.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>100</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157546</td>\n",
       "      <td>0321916603_pA-32_005.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>194</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53708</td>\n",
       "      <td>0321955048_pA20_018.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>184</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134261</td>\n",
       "      <td>032182623X_p428_001.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>188</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Name  \\\n",
       "0      166418  9780134838076_unfig17-10.png   \n",
       "1       64504       0321613376_p101_002.jpg   \n",
       "2      157546      0321916603_pA-32_005.jpg   \n",
       "3       53708       0321955048_pA20_018.jpg   \n",
       "4      134261       032182623X_p428_001.jpg   \n",
       "\n",
       "                                                Path  Height  Width  \n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     335    387  \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     100    439  \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     194    184  \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...     184    154  \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     188    560  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_images=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')\n",
    "source_images=source_images.sample(n=2000, random_state=2)\n",
    "source_images.reset_index(drop=True, inplace=True)\n",
    "source_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = (128,128,3)\n",
    "\n",
    "data_gen = ImageDataGenerator(rescale=1./255, preprocessing_function=gray)\n",
    "data_flow_dataframe_2 = data_gen.flow_from_dataframe(dataframe = source_images\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , target_size = INPUT_DIM[:2]\n",
    "                                         , batch_size = 2000\n",
    "                                         , shuffle = False\n",
    "                                         , class_mode = 'input'\n",
    "                                            )\n",
    "example_batch_2 = next(data_flow_dataframe_2)\n",
    "example_images_2 = example_batch_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Count'], index=[1,2,3,4,5])\n",
    "for i in range(1,6):\n",
    "    df.loc[i]=0\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "w, h = 5, 2000;\n",
    "Sim_score_Matrix = ([[-200 for x in range(w)] for y in range(h)]) \n",
    "z_Matrix = ([[0 for x in range(w)] for y in range(h)]) \n",
    "img_Matrix = ([[-1 for x in range(w)] for y in range(h)])\n",
    "\n",
    "\n",
    "z_points_2 = ae.encoder.predict(example_images_2)\n",
    "z=cosine_similarity(z_points_2,m)\n",
    "#print(z.shape)\n",
    "total_count=0\n",
    "for i in range(2000):\n",
    "    cnt=0\n",
    "    for l in range(5):\n",
    "        #done=0\n",
    "        if(z[i].max()>0.98):\n",
    "            k=z[i].argmax()\n",
    "            z[i][k]=0\n",
    "        elif(z[i].max()<0.98 and z[i].max()>0.8):\n",
    "            #print(z[i].max())\n",
    "            k=z[i].argmax()\n",
    "            z[i][k]=0\n",
    "            cnt+=1\n",
    "            total_count+=1\n",
    "            done=1\n",
    "        #if(done==0):\n",
    "         #   k=z[i].argmax()\n",
    "          #  z[i][k]=0\n",
    "    if(cnt>0):\n",
    "        df.loc[cnt]=df.loc[cnt]+1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Count\n",
       "1   248\n",
       "2   228\n",
       "3   294\n",
       "4   482\n",
       "5     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3514"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"520.318125pt\" version=\"1.1\" viewBox=\"0 0 542.6875 520.318125\" width=\"542.6875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 520.318125 \n",
       "L 542.6875 520.318125 \n",
       "L 542.6875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 33.2875 496.44 \n",
       "L 535.4875 496.44 \n",
       "L 535.4875 7.2 \n",
       "L 33.2875 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path clip-path=\"url(#p35ca6126e5)\" d=\"M 56.114773 496.44 \n",
       "L 132.205682 496.44 \n",
       "L 132.205682 256.701766 \n",
       "L 56.114773 256.701766 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path clip-path=\"url(#p35ca6126e5)\" d=\"M 151.228409 496.44 \n",
       "L 227.319318 496.44 \n",
       "L 227.319318 276.035495 \n",
       "L 151.228409 276.035495 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path clip-path=\"url(#p35ca6126e5)\" d=\"M 246.342045 496.44 \n",
       "L 322.432955 496.44 \n",
       "L 322.432955 212.234191 \n",
       "L 246.342045 212.234191 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path clip-path=\"url(#p35ca6126e5)\" d=\"M 341.455682 496.44 \n",
       "L 417.546591 496.44 \n",
       "L 417.546591 30.497143 \n",
       "L 341.455682 30.497143 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#p35ca6126e5)\" d=\"M 436.569318 496.44 \n",
       "L 512.660227 496.44 \n",
       "L 512.660227 496.44 \n",
       "L 436.569318 496.44 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m58566a89fe\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.160227\" xlink:href=\"#m58566a89fe\" y=\"496.44\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(90.978977 511.038437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"189.273864\" xlink:href=\"#m58566a89fe\" y=\"496.44\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(186.092614 511.038437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.3875\" xlink:href=\"#m58566a89fe\" y=\"496.44\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 3 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(281.20625 511.038437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"379.501136\" xlink:href=\"#m58566a89fe\" y=\"496.44\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(376.319886 511.038437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"474.614773\" xlink:href=\"#m58566a89fe\" y=\"496.44\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(471.433523 511.038437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m4f423f6da9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m4f423f6da9\" y=\"496.44\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(19.925 500.239219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m4f423f6da9\" y=\"399.771357\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(7.2 403.570576)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m4f423f6da9\" y=\"303.102715\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(7.2 306.901934)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m4f423f6da9\" y=\"206.434072\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(7.2 210.233291)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m4f423f6da9\" y=\"109.76543\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(7.2 113.564649)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m4f423f6da9\" y=\"13.096787\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(7.2 16.896006)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 33.2875 496.44 \n",
       "L 33.2875 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 535.4875 496.44 \n",
       "L 535.4875 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 33.2875 496.44 \n",
       "L 535.4875 496.44 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 33.2875 7.2 \n",
       "L 535.4875 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p35ca6126e5\">\n",
       "   <rect height=\"489.24\" width=\"502.2\" x=\"33.2875\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "names = ['1', '2', '3', '4', '5']\n",
    "values = [248, 228, 294, 482, 0]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.bar(names, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "w, h = 5, 2000;\n",
    "Sim_score_Matrix = ([[-200 for x in range(w)] for y in range(h)]) \n",
    "z_Matrix = ([[0 for x in range(w)] for y in range(h)]) \n",
    "img_Matrix = ([[-1 for x in range(w)] for y in range(h)])\n",
    "\n",
    "\n",
    "z_points_2 = ae.encoder.predict(example_images_2)\n",
    "z=cosine_similarity(z_points_2,m)\n",
    "#print(z.shape)\n",
    "\n",
    "for i in range(2000):\n",
    "    for l in range(5):\n",
    "        if(z[i].max()>=0.98):\n",
    "            k=z[i].argmax()\n",
    "            z[i][k]=0\n",
    "        elif(z[i].max()<0.98 and z[i].max()>0.8):\n",
    "        #if(z[i].max()>Sim_score_Matrix[i][l]):\n",
    "            Sim_score_Matrix[i][l]=z[i].max()\n",
    "            #z_Matrix[i][l]=z_points_2[z[i].argmax()]\n",
    "            img_Matrix[i][l]=z[i].argmax()\n",
    "            k=z[i].argmax()\n",
    "            z[i][k]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 181858)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-100epochs/\n"
     ]
    }
   ],
   "source": [
    "print(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5) (181858, 5)\n"
     ]
    }
   ],
   "source": [
    "print(source_images.shape, image_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9781337111348_11348_ch02_2.1_017-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9781285194790_9781285194790-551-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>550</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9781133947257_9781133947257_ch01_65-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>101</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9781305652231_52231_ch01_f022-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>24</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ceng_image205.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        Name  \\\n",
       "0           0     9781337111348_11348_ch02_2.1_017-t2.png   \n",
       "1           1      9781285194790_9781285194790-551-t2.png   \n",
       "2           2  9781133947257_9781133947257_ch01_65-t2.png   \n",
       "3           3        9781305652231_52231_ch01_f022-t2.png   \n",
       "4           4                           ceng_image205.png   \n",
       "\n",
       "                                                Path  Height  Width  \n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     131    181  \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     550    593  \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     101    184  \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...      24    188  \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     131    268  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alt Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataframe=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')\n",
    "cengage= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/cengage_production_data.csv')\n",
    "pearson= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/pearson_production_data.csv')\n",
    "wiley=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/wiley_production_data.csv')\n",
    "wiley.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "pearson.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "cengage.rename(columns={'image_names':'Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                   38344\n",
       "maths                   13844\n",
       "science                  3288\n",
       "engineering              1854\n",
       "history                  1014\n",
       "agriculture, science      848\n",
       "law, business             784\n",
       "accounting                612\n",
       "business                  561\n",
       "law                       536\n",
       "computer                  513\n",
       "language                  375\n",
       "economics, math           130\n",
       "psychology                112\n",
       "computer, business         94\n",
       "economics                  63\n",
       "business, language         55\n",
       "Name: subject_area, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cengage= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/cengage_production_data.csv')\n",
    "cengage.head()\n",
    "pd.unique(cengage[['subject_area']].values.ravel('K'))\n",
    "cengage['subject_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181858, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataframe=pd.merge(image_dataframe, cengage, on='Name', how='left')\n",
    "image_dataframe.drop(['image_path', 'alt_text_html', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, pearson, on='Name', how='left')\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text_x\"].astype(str) + \" \"+image_dataframe[\"alt_text_y\"].astype(str)\n",
    "image_dataframe['subject_area'] = image_dataframe['subject_area_x'].astype(str) + \" \"+ image_dataframe['subject_area_y'].astype(str)\n",
    "image_dataframe.drop(['alt_text_x', \"alt_text_y\", 'subject_area_x', 'subject_area_y', 'alt_text_html', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, wiley, on='Name', how='left')\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text_x\"].astype(str) + \" \"+ image_dataframe[\"alt_text_y\"].astype(str)\n",
    "image_dataframe['subject_area'] = image_dataframe['subject_area_x'].astype(str) + \" \"+ image_dataframe['subject_area_y'].astype(str)\n",
    "image_dataframe.drop(['alt_text_x', \"alt_text_y\", 'subject_area_x', 'subject_area_y', 'alt_text_html', 'image_path'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>subject_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781337111348_11348_ch02_2.1_017-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>181</td>\n",
       "      <td>The image consists of a Minitab output. Visual...</td>\n",
       "      <td>maths nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781285194790_9781285194790-551-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>550</td>\n",
       "      <td>593</td>\n",
       "      <td>A map shows the Second Iraq War. Majority popu...</td>\n",
       "      <td>history nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781133947257_9781133947257_ch01_65-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>101</td>\n",
       "      <td>184</td>\n",
       "      <td>In the Sherlock Holmes mystery The Final Solut...</td>\n",
       "      <td>maths nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781305652231_52231_ch01_f022-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>24</td>\n",
       "      <td>188</td>\n",
       "      <td>The image consists of a number line. The numbe...</td>\n",
       "      <td>maths nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ceng_image205.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>268</td>\n",
       "      <td>The image consists of a visual representation ...</td>\n",
       "      <td>maths nan nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name  \\\n",
       "0     9781337111348_11348_ch02_2.1_017-t2.png   \n",
       "1      9781285194790_9781285194790-551-t2.png   \n",
       "2  9781133947257_9781133947257_ch01_65-t2.png   \n",
       "3        9781305652231_52231_ch01_f022-t2.png   \n",
       "4                           ceng_image205.png   \n",
       "\n",
       "                                                Path  Height  Width  \\\n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     131    181   \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     550    593   \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     101    184   \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...      24    188   \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     131    268   \n",
       "\n",
       "                                            alt_text     subject_area  \n",
       "0  The image consists of a Minitab output. Visual...    maths nan nan  \n",
       "1  A map shows the Second Iraq War. Majority popu...  history nan nan  \n",
       "2  In the Sherlock Holmes mystery The Final Solut...    maths nan nan  \n",
       "3  The image consists of a number line. The numbe...    maths nan nan  \n",
       "4  The image consists of a visual representation ...    maths nan nan  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181858, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name            0\n",
       "Path            0\n",
       "Height          0\n",
       "Width           0\n",
       "alt_text        0\n",
       "subject_area    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_images=pd.merge(source_images, image_dataframe, on='Name', how='left')\n",
    "source_images.drop(['Path_y', \"Height_y\", 'Width_y', 'Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Path_x</th>\n",
       "      <th>Height_x</th>\n",
       "      <th>Width_x</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>subject_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9780134838076_unfig17-10.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>335</td>\n",
       "      <td>387</td>\n",
       "      <td>nan Product design A proceeds as follows from ...</td>\n",
       "      <td>nan canada nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0321613376_p101_002.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>100</td>\n",
       "      <td>439</td>\n",
       "      <td>nan Multiplying by 6 on both sidesUsing the di...</td>\n",
       "      <td>nan maths nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0321916603_pA-32_005.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>194</td>\n",
       "      <td>184</td>\n",
       "      <td>nan The graph of x213+y29= 1is an ellipse cent...</td>\n",
       "      <td>nan maths nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0321955048_pA20_018.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>184</td>\n",
       "      <td>154</td>\n",
       "      <td>nan A tree diagram with 16 outcomes. Branches ...</td>\n",
       "      <td>nan maths nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>032182623X_p428_001.jpg</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>188</td>\n",
       "      <td>560</td>\n",
       "      <td>nan Two-Sample T-Test for SUCCESSGROUPNMeanStD...</td>\n",
       "      <td>nan maths nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name  \\\n",
       "0  9780134838076_unfig17-10.png   \n",
       "1       0321613376_p101_002.jpg   \n",
       "2      0321916603_pA-32_005.jpg   \n",
       "3       0321955048_pA20_018.jpg   \n",
       "4       032182623X_p428_001.jpg   \n",
       "\n",
       "                                              Path_x  Height_x  Width_x  \\\n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...       335      387   \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...       100      439   \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...       194      184   \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...       184      154   \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...       188      560   \n",
       "\n",
       "                                            alt_text    subject_area  \n",
       "0  nan Product design A proceeds as follows from ...  nan canada nan  \n",
       "1  nan Multiplying by 6 on both sidesUsing the di...   nan maths nan  \n",
       "2  nan The graph of x213+y29= 1is an ellipse cent...   nan maths nan  \n",
       "3  nan A tree diagram with 16 outcomes. Branches ...   nan maths nan  \n",
       "4  nan Two-Sample T-Test for SUCCESSGROUPNMeanStD...   nan maths nan  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-100epochs/\n"
     ]
    }
   ],
   "source": [
    "print(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_IMAGE=RUN_FOLDER+'/Test_sc_0.8_0.98_trunc_text/'#.format(i)\n",
    "filename2='similar_image_'+str(2)+'.txt'\n",
    "file1=open(os.path.join(RESULT_IMAGE,filename2), \"w\")\n",
    "alt_text=image_dataframe['alt_text'][0]\n",
    "file1.write(alt_text+\" \\n\")\n",
    "file1.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_image=pd.read_csv('image_location.csv')\n",
    "import cv2\n",
    "for i in range(2000):\n",
    "    RESULT_IMAGE=RUN_FOLDER+'/Test_sc_0.8_0.98_trunc_text/{}/'.format(i)\n",
    "    for k in range(5):\n",
    "        if(img_Matrix[i][k]!=-1):\n",
    "            if not os.path.exists(RESULT_IMAGE):\n",
    "                os.mkdir(RESULT_IMAGE)\n",
    "            similar_img=cv2.imread(image_dataframe['Path'][img_Matrix[i][k]],0)\n",
    "            alt_text=image_dataframe['alt_text'][img_Matrix[i][k]].encode('utf-8')      #image_dataframe has all the images\n",
    "            score=round(Sim_score_Matrix[i][k], 3)\n",
    "            filename='similar_image_'+str(k)+'__'+str(score)+'.jpg'\n",
    "            cv2.imwrite(os.path.join(RESULT_IMAGE,filename), similar_img)\n",
    "\n",
    "            filename2='similar_image_'+str(k)+'.txt'\n",
    "            file1=open(os.path.join(RESULT_IMAGE,filename2), \"w\")\n",
    "            file1.write(str(alt_text)+\" \\n\")\n",
    "            file1.close()\n",
    "    if os.path.exists(RESULT_IMAGE):\n",
    "        img = cv2.imread(source_images['Path_x'][i],0)   #source_images has random 100 image\n",
    "        filename='original_image_'+str(i)+'.jpg'\n",
    "        cv2.imwrite(os.path.join(RESULT_IMAGE,filename), img)\n",
    "\n",
    "        alt_text_2=str(source_images['alt_text'][i]).encode('utf-8')\n",
    "        filename2='original_image_'+str(i)+'.txt'\n",
    "        file1=open(os.path.join(RESULT_IMAGE,filename2), \"w\")\n",
    "        file1.write(str(alt_text_2)+\" \\n\")\n",
    "        file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(0.9999999999999,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_IMAGE=RUN_FOLDER+'/Test_sc_0.8_0.9_trunc/0/'\n",
    "if os.path.exists(RESULT_IMAGE):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
