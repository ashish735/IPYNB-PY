{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class based AutoEncoder, Flow From Dataframe, Retraining Model, Call Backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   #https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import sys  \n",
    "sys.path.insert(0, '/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/')\n",
    "import tensorflow as tf\n",
    "from models.AE import Autoencoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/\n"
     ]
    }
   ],
   "source": [
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/'\n",
    "\n",
    "TEST_FOLDER= RUN_FOLDER+ '/Testing/'\n",
    "LOG_FOLDER= RUN_FOLDER+ '/logs'\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.mkdir(TEST_FOLDER)\n",
    "if not os.path.exists(LOG_FOLDER):\n",
    "    os.mkdir(LOG_FOLDER)\n",
    "    \n",
    "print(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataframe=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')\n",
    "INPUT_DIM = (128,128,3)\n",
    "BATCH_SIZE = 32\n",
    "NUM_IMAGES = 181858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = np.expand_dims(gray_image, axis=-1)\n",
    "    #print(gray_image.shape)\n",
    "    return gray_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181858 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(rescale=1./255, preprocessing_function=gray)\n",
    "\n",
    "data_flow_dataframe = data_gen.flow_from_dataframe(dataframe = image_dataframe\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , target_size = INPUT_DIM[:2]\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = False\n",
    "                                         , class_mode = 'input'\n",
    "                                         , color_mode = 'rgb'\n",
    "                                         #, save_to_dir = RUN_FOLDER\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "ae = Autoencoder(\n",
    "                input_dim = INPUT_DIM\n",
    "                , encoder_conv_filters=[32,48,64, 64]\n",
    "                , encoder_conv_kernel_size=[3,3,3,3]\n",
    "                , encoder_conv_strides=[2,2,2,2]\n",
    "                , decoder_conv_t_filters=[64,48,32,3]\n",
    "                , decoder_conv_t_kernel_size=[3,3,3,3]\n",
    "                , decoder_conv_t_strides=[2,2,2,2]\n",
    "                , z_dim=200\n",
    "                , use_batch_norm=True\n",
    "                , use_dropout=True)\n",
    "\n",
    "ae.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "#R_LOSS_FACTOR = 10000\n",
    "EPOCHS = 100\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.compile(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_input_shape': (None, 200), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'decoder_input'}\n",
      "InputLayer\n",
      "<class 'list'>\n",
      "{'name': 'dense', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'units': 4096, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "Dense\n",
      "<class 'list'>\n",
      "{'name': 'reshape', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'target_shape': (8, 8, 64)}\n",
      "Reshape\n",
      "<class 'list'>\n",
      "{'name': 'decoder_conv_t_0', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'filters': 64, 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'output_padding': None}\n",
      "Conv2DTranspose\n",
      "<class 'list'>\n",
      "{'name': 'leaky_re_lu_4', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'alpha': 0.30000001192092896}\n",
      "LeakyReLU\n",
      "<class 'list'>\n",
      "{'name': 'batch_normalization_4', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}\n",
      "BatchNormalization\n",
      "<class 'list'>\n",
      "{'name': 'dropout_4', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'rate': 0.25, 'noise_shape': None, 'seed': None}\n",
      "Dropout\n",
      "<class 'list'>\n",
      "{'name': 'decoder_conv_t_1', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'filters': 48, 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'output_padding': None}\n",
      "Conv2DTranspose\n",
      "<class 'list'>\n",
      "{'name': 'leaky_re_lu_5', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'alpha': 0.30000001192092896}\n",
      "LeakyReLU\n",
      "<class 'list'>\n",
      "{'name': 'batch_normalization_5', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}\n",
      "BatchNormalization\n",
      "<class 'list'>\n",
      "{'name': 'dropout_5', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'rate': 0.25, 'noise_shape': None, 'seed': None}\n",
      "Dropout\n",
      "<class 'list'>\n",
      "{'name': 'decoder_conv_t_2', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'filters': 32, 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'output_padding': None}\n",
      "Conv2DTranspose\n",
      "<class 'list'>\n",
      "{'name': 'leaky_re_lu_6', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'alpha': 0.30000001192092896}\n",
      "LeakyReLU\n",
      "<class 'list'>\n",
      "{'name': 'batch_normalization_6', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}\n",
      "BatchNormalization\n",
      "<class 'list'>\n",
      "{'name': 'dropout_6', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'rate': 0.25, 'noise_shape': None, 'seed': None}\n",
      "Dropout\n",
      "<class 'list'>\n",
      "{'name': 'decoder_conv_t_3', 'trainable': True, 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}}, 'filters': 3, 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'output_padding': None}\n",
      "Conv2DTranspose\n",
      "<class 'list'>\n",
      "{'name': 'Activation', 'trainable': True, 'dtype': 'float32', 'activation': 'sigmoid'}\n",
      "Activation\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for layer in ae.decoder.layers:\n",
    "    #if(layer.get_config()['name']=='Activation'):\n",
    "        print(layer.get_config())\n",
    "        print(layer.__class__.__name__)\n",
    "        print(type(layer.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 5683.0625 steps\n",
      "Epoch 1/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0196\n",
      "Epoch 00001: loss improved from inf to 0.01961, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00001: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 263s 46ms/step - loss: 0.0196\n",
      "Epoch 2/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0178\n",
      "Epoch 00002: loss improved from 0.01961 to 0.01776, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00002: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 262s 46ms/step - loss: 0.0178\n",
      "Epoch 3/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00003: loss improved from 0.01776 to 0.01662, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00003: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 263s 46ms/step - loss: 0.0166\n",
      "Epoch 4/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00004: loss improved from 0.01662 to 0.01602, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00004: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 238s 42ms/step - loss: 0.0160\n",
      "Epoch 5/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00005: loss improved from 0.01602 to 0.01563, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00005: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 244s 43ms/step - loss: 0.0156\n",
      "Epoch 6/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0154\n",
      "Epoch 00006: loss improved from 0.01563 to 0.01536, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00006: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 240s 42ms/step - loss: 0.0154\n",
      "Epoch 7/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00007: loss improved from 0.01536 to 0.01515, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00007: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 257s 45ms/step - loss: 0.0152\n",
      "Epoch 8/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00008: loss improved from 0.01515 to 0.01501, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00008: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 298s 52ms/step - loss: 0.0150\n",
      "Epoch 9/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00009: loss improved from 0.01501 to 0.01486, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00009: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 259s 46ms/step - loss: 0.0149\n",
      "Epoch 10/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00010: loss improved from 0.01486 to 0.01478, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00010: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 261s 46ms/step - loss: 0.0148\n",
      "Epoch 11/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0147\n",
      "Epoch 00011: loss improved from 0.01478 to 0.01468, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00011: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 265s 47ms/step - loss: 0.0147\n",
      "Epoch 12/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0146\n",
      "Epoch 00012: loss improved from 0.01468 to 0.01458, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00012: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 267s 47ms/step - loss: 0.0146\n",
      "Epoch 13/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0145\n",
      "Epoch 00013: loss improved from 0.01458 to 0.01451, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00013: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 266s 47ms/step - loss: 0.0145\n",
      "Epoch 14/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0144\n",
      "Epoch 00014: loss improved from 0.01451 to 0.01443, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00014: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 264s 46ms/step - loss: 0.0144\n",
      "Epoch 15/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0144\n",
      "Epoch 00015: loss improved from 0.01443 to 0.01438, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00015: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 264s 46ms/step - loss: 0.0144\n",
      "Epoch 16/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0144\n",
      "Epoch 00016: loss improved from 0.01438 to 0.01435, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00016: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 261s 46ms/step - loss: 0.0144\n",
      "Epoch 17/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00017: loss improved from 0.01435 to 0.01430, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00017: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 268s 47ms/step - loss: 0.0143\n",
      "Epoch 18/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00018: loss improved from 0.01430 to 0.01426, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00018: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 271s 48ms/step - loss: 0.0143\n",
      "Epoch 19/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00019: loss improved from 0.01426 to 0.01423, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00019: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 276s 49ms/step - loss: 0.0142\n",
      "Epoch 20/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00020: loss improved from 0.01423 to 0.01421, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00020: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 270s 48ms/step - loss: 0.0142\n",
      "Epoch 21/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00021: loss improved from 0.01421 to 0.01419, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00021: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 266s 47ms/step - loss: 0.0142\n",
      "Epoch 22/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00022: loss improved from 0.01419 to 0.01416, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00022: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 269s 47ms/step - loss: 0.0142\n",
      "Epoch 23/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00023: loss improved from 0.01416 to 0.01413, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00023: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 266s 47ms/step - loss: 0.0141\n",
      "Epoch 24/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00024: loss improved from 0.01413 to 0.01411, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00024: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 278s 49ms/step - loss: 0.0141\n",
      "Epoch 25/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00025: loss improved from 0.01411 to 0.01409, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00025: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 270s 48ms/step - loss: 0.0141\n",
      "Epoch 26/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00026: loss improved from 0.01409 to 0.01409, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00026: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 276s 49ms/step - loss: 0.0141\n",
      "Epoch 27/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00027: loss improved from 0.01409 to 0.01407, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00027: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 278s 49ms/step - loss: 0.0141\n",
      "Epoch 28/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0141- ETA: 1 - ETA: 0s - loss:\n",
      "Epoch 00028: loss improved from 0.01407 to 0.01405, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00028: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 270s 47ms/step - loss: 0.0141\n",
      "Epoch 29/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00029: loss improved from 0.01405 to 0.01404, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00029: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 275s 48ms/step - loss: 0.0140\n",
      "Epoch 30/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00030: loss improved from 0.01404 to 0.01403, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00030: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 268s 47ms/step - loss: 0.0140\n",
      "Epoch 31/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00031: loss improved from 0.01403 to 0.01402, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00031: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 279s 49ms/step - loss: 0.0140\n",
      "Epoch 32/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00032: loss improved from 0.01402 to 0.01401, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00032: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 277s 49ms/step - loss: 0.0140\n",
      "Epoch 33/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00033: loss improved from 0.01401 to 0.01400, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00033: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 279s 49ms/step - loss: 0.0140\n",
      "Epoch 34/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00034: loss improved from 0.01400 to 0.01398, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00034: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 276s 48ms/step - loss: 0.0140\n",
      "Epoch 35/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00035: loss improved from 0.01398 to 0.01397, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00035: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 272s 48ms/step - loss: 0.0140\n",
      "Epoch 36/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00036: loss did not improve from 0.01397\n",
      "\n",
      "Epoch 00036: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 276s 49ms/step - loss: 0.0140\n",
      "Epoch 37/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00037: loss did not improve from 0.01397\n",
      "\n",
      "Epoch 00037: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 271s 48ms/step - loss: 0.0140\n",
      "Epoch 38/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00038: loss improved from 0.01397 to 0.01394, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/model.h5\n",
      "\n",
      "Epoch 00038: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 279s 49ms/step - loss: 0.0139\n",
      "Epoch 39/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00039: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00039: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 276s 49ms/step - loss: nan\n",
      "Epoch 40/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00040: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00040: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 281s 50ms/step - loss: nan\n",
      "Epoch 41/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00041: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00041: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 278s 49ms/step - loss: nan\n",
      "Epoch 42/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00042: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00042: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 275s 48ms/step - loss: nan\n",
      "Epoch 43/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00043: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00043: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 282s 50ms/step - loss: nan\n",
      "Epoch 44/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan E\n",
      "Epoch 00044: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00044: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 278s 49ms/step - loss: nan\n",
      "Epoch 45/100\n",
      "5683/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00045: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00045: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 283s 50ms/step - loss: nan\n",
      "Epoch 46/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00046: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00046: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 280s 49ms/step - loss: nan\n",
      "Epoch 47/100\n",
      "5682/5683 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 00047: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00047: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "5684/5683 [==============================] - 284s 50ms/step - loss: nan\n",
      "Epoch 48/100\n",
      " 961/5683 [====>.........................] - ETA: 3:55 - loss: nan\n",
      "Epoch 00048: loss did not improve from 0.01394\n",
      "\n",
      "Epoch 00048: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/weights.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      " 962/5683 [====>.........................] - ETA: 3:56 - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-09476f536177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mrun_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUN_FOLDER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINITIAL_EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/deep_learning/data/alt_text_data_all/models/models/AE.py\u001b[0m in \u001b[0;36mtrain_with_generator\u001b[0;34m(self, data_flow, epochs, steps_per_epoch, run_folder, print_every_n_batches, initial_epoch, lr_decay)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             )\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mcalled_without_tracing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calls_per_tracings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calls_per_tracings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calls_per_tracings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ae.train_with_generator(     \n",
    "    data_flow_dataframe\n",
    "    , epochs = EPOCHS\n",
    "    , steps_per_epoch = NUM_IMAGES / BATCH_SIZE\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P40 (UUID: GPU-532949dc-9b98-a865-e163-da1dce6470d7)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "#R_LOSS_FACTOR = 10000\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def r_loss(y_true, y_pred):\n",
    "            return K.mean(K.square(y_true - y_pred), axis = [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-500epochs/'\n",
    "\n",
    "TEST_FOLDER= RUN_FOLDER+ '/Testing/'\n",
    "LOG_FOLDER= RUN_FOLDER+ '/logs'\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.mkdir(TEST_FOLDER)\n",
    "if not os.path.exists(LOG_FOLDER):\n",
    "    os.mkdir(LOG_FOLDER)\n",
    "    \n",
    "print(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model using keras load_model library\n",
    "new_ae = load_model(\"/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/grey/AE-300epochs/model.h5\", \\\n",
    "                     custom_objects={'r_loss':r_loss})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "#R_LOSS_FACTOR = 10000\n",
    "BATCH_SIZE =1000\n",
    "EPOCHS = 200\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = np.expand_dims(gray_image, axis=-1)\n",
    "    #print(gray_image.shape)\n",
    "    return gray_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "image_dataframe=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')\n",
    "INPUT_DIM = (128,128,3)\n",
    "BATCH_SIZE = 64\n",
    "NUM_IMAGES = 181858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181858 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "data_gen = ImageDataGenerator(rescale=1./255 , preprocessing_function=gray)\n",
    "data_flow_dataframe = data_gen.flow_from_dataframe(dataframe = image_dataframe\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , target_size = INPUT_DIM[:2]\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = False\n",
    "                                         , class_mode = 'input'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
    "import os\n",
    "from datetime import datetime\n",
    "logdir = os.path.join(RUN_FOLDER, \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "#checkpoint_filepath=os.path.join(run_folder, \"model.h5\")\n",
    "checkpoint1 = ModelCheckpoint(os.path.join(RUN_FOLDER, \"model.h5\"), monitor='loss',  save_best_only=True, verbose=1, mode= 'min')\n",
    "checkpoint2 = ModelCheckpoint(os.path.join(RUN_FOLDER, 'weights.h5'), save_weights_only = True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, mode='min', verbose=1)\n",
    "EarlyStop = EarlyStopping(monitor='loss', min_delta=0.0001, patience=31, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint1, checkpoint2, reduce_lr, tensorboard_callback, EarlyStop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/200\n",
      " 151/2841 [>.............................] - ETA: 3:57 - loss: 0.0136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 629/2841 [=====>........................] - ETA: 2:56 - loss: 0.0140"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d54746c3755b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_IMAGES\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_ae.fit_generator(\n",
    "            data_flow_dataframe\n",
    "            , shuffle = True\n",
    "            , epochs = EPOCHS\n",
    "            , initial_epoch = INITIAL_EPOCH\n",
    "            , callbacks = callbacks_list\n",
    "            , steps_per_epoch=NUM_IMAGES / BATCH_SIZE\n",
    "            , workers= 7\n",
    "            , use_multiprocessing=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
