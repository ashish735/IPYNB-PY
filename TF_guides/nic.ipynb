{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 122s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>subject_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781337111348_11348_ch02_2.1_017-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>181</td>\n",
       "      <td>The image consists of a Minitab output. Visual...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781285194790_9781285194790-551-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>550</td>\n",
       "      <td>593</td>\n",
       "      <td>A map shows the Second Iraq War. Majority popu...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781133947257_9781133947257_ch01_65-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>101</td>\n",
       "      <td>184</td>\n",
       "      <td>In the Sherlock Holmes mystery The Final Solut...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781305652231_52231_ch01_f022-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>24</td>\n",
       "      <td>188</td>\n",
       "      <td>The image consists of a number line. The numbe...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ceng_image205.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>268</td>\n",
       "      <td>The image consists of a visual representation ...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name  \\\n",
       "0     9781337111348_11348_ch02_2.1_017-t2.png   \n",
       "1      9781285194790_9781285194790-551-t2.png   \n",
       "2  9781133947257_9781133947257_ch01_65-t2.png   \n",
       "3        9781305652231_52231_ch01_f022-t2.png   \n",
       "4                           ceng_image205.png   \n",
       "\n",
       "                                                Path  Height  Width  \\\n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     131    181   \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     550    593   \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     101    184   \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...      24    188   \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     131    268   \n",
       "\n",
       "                                            alt_text subject_area  \n",
       "0  The image consists of a Minitab output. Visual...        maths  \n",
       "1  A map shows the Second Iraq War. Majority popu...      history  \n",
       "2  In the Sherlock Holmes mystery The Final Solut...        maths  \n",
       "3  The image consists of a number line. The numbe...        maths  \n",
       "4  The image consists of a visual representation ...        maths  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_dataframe=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/image_location_venky.csv')\n",
    "cengage= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/cengage_production_data.csv')\n",
    "pearson= pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/pearson_production_data.csv')\n",
    "wiley=pd.read_csv('/home/pytorch_ashish/deep_learning/data/alt_text_data_all/wiley_production_data.csv')\n",
    "wiley.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "pearson.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "cengage.rename(columns={'image_names':'Name'}, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, cengage, on='Name', how='left')\n",
    "image_dataframe.drop(['image_path', 'alt_text_html', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, pearson, on='Name', how='left')\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text_x\"].astype(str) + \" \"+image_dataframe[\"alt_text_y\"].astype(str)\n",
    "image_dataframe['subject_area'] = image_dataframe['subject_area_x'].astype(str) + \" \"+ image_dataframe['subject_area_y'].astype(str)\n",
    "image_dataframe.drop(['alt_text_x', \"alt_text_y\", 'subject_area_x', 'subject_area_y', 'alt_text_html', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "image_dataframe=pd.merge(image_dataframe, wiley, on='Name', how='left')\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text_x\"].astype(str) + \" \"+ image_dataframe[\"alt_text_y\"].astype(str)\n",
    "image_dataframe['subject_area'] = image_dataframe['subject_area_x'].astype(str) + \" \"+ image_dataframe['subject_area_y'].astype(str)\n",
    "image_dataframe.drop(['alt_text_x', \"alt_text_y\", 'subject_area_x', 'subject_area_y', 'alt_text_html', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "banned = ['nan']\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])\n",
    "image_dataframe[\"subject_area\"] = image_dataframe[\"subject_area\"].apply(f)\n",
    "image_dataframe[\"alt_text\"] = image_dataframe[\"alt_text\"].apply(f)\n",
    "image_dataframe = image_dataframe[image_dataframe.subject_area != '']\n",
    "image_dataframe.subject_area=image_dataframe.subject_area.replace({\"other other\": \"others\", \"other\": \"others\"})\n",
    "image_dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "include = ['maths', 'science', 'others', 'emss', 'canada', 'engineering', 'business', 'history']\n",
    "indexNames=[]\n",
    "for i in range(image_dataframe.shape[0]):\n",
    "    if(image_dataframe.loc[i]['subject_area'] not in include):\n",
    "        indexNames.append(i)\n",
    "image_dataframe.drop(labels=indexNames, inplace=True)\n",
    "image_dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "image_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173778, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maths          146839\n",
       "science         10820\n",
       "others           5116\n",
       "emss             3602\n",
       "canada           2852\n",
       "engineering      1854\n",
       "business         1681\n",
       "history          1014\n",
       "Name: subject_area, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe['subject_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['maths', 'history', 'science', 'engineering', 'business', 'others',\n",
       "       'emss', 'canada'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.subject_area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name            0\n",
       "Path            0\n",
       "Height          0\n",
       "Width           0\n",
       "alt_text        0\n",
       "subject_area    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= image_dataframe.loc[:,:]\n",
    "y= image_dataframe.loc[:,'subject_area']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>subject_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781337111348_11348_ch02_2.1_017-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>181</td>\n",
       "      <td>The image consists of a Minitab output. Visual...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781285194790_9781285194790-551-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>550</td>\n",
       "      <td>593</td>\n",
       "      <td>A map shows the Second Iraq War. Majority popu...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781133947257_9781133947257_ch01_65-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>101</td>\n",
       "      <td>184</td>\n",
       "      <td>In the Sherlock Holmes mystery The Final Solut...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781305652231_52231_ch01_f022-t2.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>24</td>\n",
       "      <td>188</td>\n",
       "      <td>The image consists of a number line. The numbe...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ceng_image205.png</td>\n",
       "      <td>/home/pytorch_ashish/deep_learning/data/alt_te...</td>\n",
       "      <td>131</td>\n",
       "      <td>268</td>\n",
       "      <td>The image consists of a visual representation ...</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name  \\\n",
       "0     9781337111348_11348_ch02_2.1_017-t2.png   \n",
       "1      9781285194790_9781285194790-551-t2.png   \n",
       "2  9781133947257_9781133947257_ch01_65-t2.png   \n",
       "3        9781305652231_52231_ch01_f022-t2.png   \n",
       "4                           ceng_image205.png   \n",
       "\n",
       "                                                Path  Height  Width  \\\n",
       "0  /home/pytorch_ashish/deep_learning/data/alt_te...     131    181   \n",
       "1  /home/pytorch_ashish/deep_learning/data/alt_te...     550    593   \n",
       "2  /home/pytorch_ashish/deep_learning/data/alt_te...     101    184   \n",
       "3  /home/pytorch_ashish/deep_learning/data/alt_te...      24    188   \n",
       "4  /home/pytorch_ashish/deep_learning/data/alt_te...     131    268   \n",
       "\n",
       "                                            alt_text subject_area  \n",
       "0  The image consists of a Minitab output. Visual...        maths  \n",
       "1  A map shows the Second Iraq War. Majority popu...      history  \n",
       "2  In the Sherlock Holmes mystery The Final Solut...        maths  \n",
       "3  The image consists of a number line. The numbe...        maths  \n",
       "4  The image consists of a visual representation ...        maths  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      maths\n",
       "1    history\n",
       "2      maths\n",
       "3      maths\n",
       "4      maths\n",
       "Name: subject_area, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130333, 6) (130333,) (43445, 6) (43445,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = np.expand_dims(gray_image, axis=-1)\n",
    "    return gray_image\n",
    "INPUT_DIM = (128,128,3)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130333 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "traindata = trdata.flow_from_dataframe(dataframe = X_train\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = True\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43445 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "tsdata = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "testdata = tsdata.flow_from_dataframe(dataframe = X_test\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = True\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggmodel.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggmodel = VGG16(weights='imagenet', include_top=True)\n",
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7ff23dd12080>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff29046ab00>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff23ae64898>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff23ae64630>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff239f670b8>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff239f85f98>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff239f85f60>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff239f8c240>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff239f93d30>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff239f93cf8>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff235d35d30>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff235d43438>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff235d36780>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff235d36a20>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff235d51e48>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff235d516a0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff235d3df28>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff235d3ce48>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff235d3ce10>\n"
     ]
    }
   ],
   "source": [
    "for layers in (vggmodel.layers)[:19]:\n",
    "    print(layers)\n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "X= vggmodel.layers[-2].output\n",
    "predictions = Dense(8, activation=\"softmax\")(X)\n",
    "model_final = Model(vggmodel.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 32776     \n",
      "=================================================================\n",
      "Total params: 134,293,320\n",
      "Trainable params: 119,578,632\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, TerminateOnNaN, EarlyStopping\n",
    "import os\n",
    "from datetime import datetime\n",
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/' \n",
    "logdir = os.path.join(RUN_FOLDER, \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "#checkpoint_filepath=os.path.join(run_folder, \"model.h5\")\n",
    "checkpoint1 = ModelCheckpoint(os.path.join(RUN_FOLDER, \"model.h5\"), monitor='val_loss',  save_best_only=True, verbose=1, mode= 'min')\n",
    "checkpoint2 = ModelCheckpoint(os.path.join(RUN_FOLDER, 'weights.h5'), save_weights_only = True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, mode='min', verbose=1)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=31, verbose=1, mode='min', restore_best_weights=True)\n",
    "terminate = TerminateOnNaN()\n",
    "\n",
    "callbacks_list = [checkpoint1, checkpoint2, reduce_lr, tensorboard_callback, EarlyStop, terminate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES= X_train.shape[0]\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=100\n",
    "VAL_NUM_IMAGES = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43445 130333\n"
     ]
    }
   ],
   "source": [
    "print(VAL_NUM_IMAGES, NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(lr=0.0001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 4072.90625 steps, validate for 1357.65625 steps\n",
      "Epoch 1/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.3427 - accuracy: 0.8941\n",
      "Epoch 00001: val_loss improved from inf to 0.33799, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/model.h5\n",
      "\n",
      "Epoch 00001: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 1639s 402ms/step - loss: 0.3427 - accuracy: 0.8941 - val_loss: 0.3380 - val_accuracy: 0.8933\n",
      "Epoch 2/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9226\n",
      "Epoch 00002: val_loss improved from 0.33799 to 0.27053, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/model.h5\n",
      "\n",
      "Epoch 00002: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 1399s 343ms/step - loss: 0.2411 - accuracy: 0.9226 - val_loss: 0.2705 - val_accuracy: 0.9149\n",
      "Epoch 3/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9435\n",
      "Epoch 00003: val_loss improved from 0.27053 to 0.26549, saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/model.h5\n",
      "\n",
      "Epoch 00003: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 1471s 361ms/step - loss: 0.1720 - accuracy: 0.9435 - val_loss: 0.2655 - val_accuracy: 0.9194\n",
      "Epoch 4/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9595\n",
      "Epoch 00004: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00004: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 646s 159ms/step - loss: 0.1200 - accuracy: 0.9595 - val_loss: 0.2857 - val_accuracy: 0.9189\n",
      "Epoch 5/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9702\n",
      "Epoch 00005: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00005: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "4073/4072 [==============================] - 640s 157ms/step - loss: 0.0862 - accuracy: 0.9702 - val_loss: 0.3433 - val_accuracy: 0.9182\n",
      "Epoch 6/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9917\n",
      "Epoch 00006: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00006: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 641s 157ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.3790 - val_accuracy: 0.9262\n",
      "Epoch 7/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9953\n",
      "Epoch 00007: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00007: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "4073/4072 [==============================] - 641s 157ms/step - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.4198 - val_accuracy: 0.9268\n",
      "Epoch 8/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 00008: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00008: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 640s 157ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.4467 - val_accuracy: 0.9271\n",
      "Epoch 9/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9990\n",
      "Epoch 00009: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00009: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "4073/4072 [==============================] - 640s 157ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.4768 - val_accuracy: 0.9282\n",
      "Epoch 10/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00010: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00010: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "4073/4072 [==============================] - 639s 157ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.4847 - val_accuracy: 0.9283\n",
      "Epoch 11/100\n",
      "4072/4072 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00011: val_loss did not improve from 0.26549\n",
      "\n",
      "Epoch 00011: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "4073/4072 [==============================] - 639s 157ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.4977 - val_accuracy: 0.9280\n",
      "Epoch 12/100\n",
      " 270/4072 [>.............................] - ETA: 7:48 - loss: 0.0028 - accuracy: 0.9994WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "\n",
      "Epoch 00012: saving model to /home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/weights.h5\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      " 270/4072 [>.............................] - ETA: 8:26 - loss: 0.0028 - accuracy: 0.9994"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d52f2dbb2cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mVAL_NUM_IMAGES\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_final.fit(\n",
    "            x = traindata\n",
    "            , shuffle = True\n",
    "            , epochs = EPOCHS\n",
    "            , initial_epoch = 0\n",
    "            , callbacks = callbacks_list\n",
    "            , steps_per_epoch=NUM_IMAGES / BATCH_SIZE\n",
    "            , workers= 7\n",
    "            , use_multiprocessing=False\n",
    "            , validation_data= testdata\n",
    "            , validation_steps= VAL_NUM_IMAGES / BATCH_SIZE\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=RUN_FOLDER+'val_acc_9280_model.h5'\n",
    "model_final.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "#R_LOSS_FACTOR = 10000\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOLDER='/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model(\"/home/pytorch_ashish/deep_learning/data/alt_text_data_all/models/NIC_model/val_acc_9280_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6e-07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=K.eval(new_model.optimizer.lr)\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 32776     \n",
      "=================================================================\n",
      "Total params: 134,293,320\n",
      "Trainable params: 119,578,632\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layers in (new_model.layers)[:]:\n",
    "    try:\n",
    "        print(layers.get_config()['trainable'])  #\n",
    "    except:\n",
    "        pass\n",
    "    #layers.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in (new_model.layers)[11:]:\n",
    "    try:\n",
    "        if(layers.get_config()['trainable']==False):\n",
    "            layers.trainable = True#\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 32776     \n",
      "=================================================================\n",
      "Total params: 134,293,320\n",
      "Trainable params: 132,557,832\n",
      "Non-trainable params: 1,735,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the ouput dtype and shape for the image data generator to include it in tf.data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130333 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "tr_images, tr_labels= next(trdata.flow_from_dataframe(dataframe = X_train\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = True\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (32, 224, 224, 3)\n",
      "float32 (32, 8)\n"
     ]
    }
   ],
   "source": [
    "print(tr_images.dtype, tr_images.shape)\n",
    "print(tr_labels.dtype, tr_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43445 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "images, labels= next(tsdata.flow_from_dataframe(dataframe = X_test\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = True\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (32, 224, 224, 3)\n",
      "float32 (32, 8)\n"
     ]
    }
   ],
   "source": [
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tf.data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130333 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "traindata=trdata.flow_from_dataframe(dataframe = X_train\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = True\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43445 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "tsdata = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "testdata=tsdata.flow_from_dataframe(dataframe = X_test\n",
    "                                         , directory = None\n",
    "                                         , x_col='Path'\n",
    "                                         , y_col='subject_area'\n",
    "                                         , target_size = (224,224)\n",
    "                                         , batch_size = BATCH_SIZE\n",
    "                                         , shuffle = True\n",
    "                                         #, classes = y_train_en\n",
    "                                         , class_mode = 'categorical'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130333, 6)\n",
      "4073\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(int(round(X_train.shape[0]/BATCH_SIZE ,0)))\n",
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43445, 6)\n",
      "1358\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(int(round(X_test.shape[0]/BATCH_SIZE ,0)))\n",
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callable_gen():\n",
    "    #for i in range(int(round(X_train.shape[0]/BATCH_SIZE ,0))):\n",
    "    img, label =next(traindata)\n",
    "    k = (img, label)\n",
    "    yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callable_gen_test():\n",
    "    #for i in range(int(round(X_test.shape[0]/BATCH_SIZE ,0))):\n",
    "    img, label =next(testdata)\n",
    "    kt = (img, label)\n",
    "    yield kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ((32, 224, 224, 3), (32, 8)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe = X_test\n",
    "#directory = None\n",
    "#x_col='Path'\n",
    "#y_col='subject_area'\n",
    "#target_size = (224,224)\n",
    "#batch_size = BATCH_SIZE\n",
    "#shuffle = True\n",
    "#, classes = y_train_en\n",
    "#class_mode = 'categorical'\n",
    "#trdata = ImageDataGenerator(rescale=1./255,  preprocessing_function=gray)\n",
    "ds = tf.data.Dataset.from_generator(\n",
    "    callable_gen,\n",
    "    output_types=(tf.float32, tf.float32), \n",
    "    output_shapes=([32,224,224,3], [32,8])\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ((32, 224, 224, 3), (32, 8)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = tf.data.Dataset.from_generator(\n",
    "    callable_gen_test,\n",
    "    output_types=(tf.float32, tf.float32), \n",
    "    output_shapes=([32,224,224,3], [32,8])\n",
    ")\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ((32, 224, 224, 3), (32, 8)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mui=int(round(X_train.shape[0]/BATCH_SIZE ,0))\n",
    "muv=int(round(X_test.shape[0]/BATCH_SIZE ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b88f29998fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcallable_gen_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmuv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    531\u001b[0m                      'at same time.')\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   \u001b[0;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 998\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    999\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "new_model.fit(\n",
    "            x = callable_gen\n",
    "            , shuffle = True\n",
    "            , epochs = 100\n",
    "            , initial_epoch = 0\n",
    "            , callbacks = callbacks_list\n",
    "            , steps_per_epoch=mui\n",
    "            , workers= 7\n",
    "            , use_multiprocessing=True\n",
    "            , validation_data= callable_gen_test\n",
    "            , validation_steps= muv\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
