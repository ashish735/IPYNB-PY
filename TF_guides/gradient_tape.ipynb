{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the TensorFlow 2.0 release, we now have the GradientTape\n",
    "function, which makes it easier than ever to write custom training loops for both TensorFlow and Keras models, thanks to automatic differentiation.\n",
    "\n",
    "Whether you’re a deep learning practitioner or a seasoned researcher, you should learn how to use the GradientTape\n",
    "function — it allows you to create custom training loops for models implemented in Keras’ easy-to-use API, giving you the best of both worlds. You just can’t beat that combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation (also called computational differentiation) refers to a set of techniques that can automatically compute the derivative of a function by repeatedly applying the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Automatic differentiation exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.).\n",
    "\n",
    "By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing custom training loops with Keras and TensorFlow, you to need to define, at a bare minimum, four components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 1: The model architecture\n",
    "\n",
    "Component 2: The loss function used when computing the model loss\n",
    "\n",
    "Component 3: The optimizer used to update the model weights\n",
    "\n",
    "Component 4: The step function that encapsulates the forward and backward pass of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with our imports from TensorFlow 2.0 and NumPy.\n",
    "\n",
    "If you inspect carefully, you won’t see GradientTape\n",
    "; we can access it via tf.GradientTape\n",
    ". We will be using the MNIST dataset (mnist\n",
    ") for our example in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go ahead and build our model using TensorFlow/Keras’ Sequential API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(width, height, depth, classes):\n",
    "\t# initialize the input shape and channels dimension to be\n",
    "\t# \"channels last\" ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\n",
    "\t# build the model using Keras' Sequential API\n",
    "\tmodel = Sequential([\n",
    "\t\t# CONV => RELU => BN => POOL layer set\n",
    "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
    "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "\t\t# (CONV => RELU => BN) * 3 => POOL layer set\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tFlatten(),\n",
    "\t\tDense(256),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(),\n",
    "\t\tDropout(0.5),\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tDense(classes),\n",
    "\t\tActivation(\"softmax\")\n",
    "\t])\n",
    "\n",
    "\t# return the built model to the calling function\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our build_model\n",
    "function used to construct the model architecture (Component #1 of creating a custom training loop). The function accepts the shape parameters for our data:\n",
    "\n",
    "    width\n",
    "    and height\n",
    "    : The spatial dimensions of each input image\n",
    "    depth\n",
    "    : The number of channels for our images (1 for grayscale as in the case of MNIST or 3 for RGB color images)\n",
    "    classes\n",
    "    : The number of unique class labels in our dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s work on Components 2, 3, and 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, y):\n",
    "\t# keep track of our gradients\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\t# make a prediction using the model and then calculate the\n",
    "\t\t# loss\n",
    "\t\tpred = model(X)\n",
    "\t\tloss = categorical_crossentropy(y, pred)\n",
    "\n",
    "\t# calculate the gradients using our tape and then update the\n",
    "\t# model weights\n",
    "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
    "\topt.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our step\n",
    "function accepts training images X\n",
    "and their corresponding class labels y\n",
    "(in our example, MNIST images and labels).\n",
    "\n",
    "Now let’s record our gradients (fancy word for derivative) by:\n",
    "\n",
    "    Gathering predictions on our training data using our model\n",
    "    pred = model(X)\n",
    "    Computing the loss\n",
    "    (Component #2 of creating a custom training loop) on loss = categorical_crossentropy(y, pred)\n",
    "\n",
    "We then calculate our gradients using tape.gradients\n",
    "and by passing our loss\n",
    "and trainable variables grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "We use our optimizer to update the model weights using the gradients on opt.apply_gradients(zip(grads, model.trainable_variables)) (Component #3).\n",
    "\n",
    "The step\n",
    "function as a whole rounds out Component #4, encapsulating our forward and backward pass of data using our GradientTape\n",
    "and then updating our model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both our build_model\n",
    "and step\n",
    "functions defined, now we’ll prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading MNIST dataset...\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 2s 0us/step\n"
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# one-hot encode the labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data in hand and ready to go, we’ll build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] creating model...\n"
    }
   ],
   "source": [
    "# build our model and initialize our optimizer\n",
    "print(\"[INFO] creating model...\")\n",
    "model = build_model(28, 28, 1, 10)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now ready to train our model with our GradientTape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] starting epoch 1/25...took 0.9399 minutes\n[INFO] starting epoch 2/25...took 0.7805 minutes\n[INFO] starting epoch 3/25...took 0.7648 minutes\n[INFO] starting epoch 4/25...took 0.8133 minutes\n[INFO] starting epoch 5/25...took 0.7757 minutes\n[INFO] starting epoch 6/25...took 0.7758 minutes\n[INFO] starting epoch 7/25...took 0.775 minutes\n[INFO] starting epoch 8/25...took 0.7712 minutes\n[INFO] starting epoch 9/25...took 0.782 minutes\n[INFO] starting epoch 10/25...took 0.7765 minutes\n[INFO] starting epoch 11/25...took 0.7596 minutes\n[INFO] starting epoch 12/25...took 0.7576 minutes\n[INFO] starting epoch 13/25...took 0.7569 minutes\n[INFO] starting epoch 14/25...took 0.7557 minutes\n[INFO] starting epoch 15/25...took 0.7568 minutes\n[INFO] starting epoch 16/25...took 0.78 minutes\n[INFO] starting epoch 17/25...took 0.7532 minutes\n[INFO] starting epoch 18/25...took 0.7729 minutes\n[INFO] starting epoch 19/25...took 0.7836 minutes\n[INFO] starting epoch 20/25...took 0.7806 minutes\n[INFO] starting epoch 21/25...took 0.7857 minutes\n[INFO] starting epoch 22/25...took 0.7682 minutes\n[INFO] starting epoch 23/25...took 0.7576 minutes\n[INFO] starting epoch 24/25...took 0.7605 minutes\n[INFO] starting epoch 25/25...took 0.7595 minutes\n"
    }
   ],
   "source": [
    "# compute the number of batch updates per epoch\n",
    "numUpdates = int(trainX.shape[0] / BS)\n",
    "\n",
    "# loop over the number of epochs\n",
    "for epoch in range(0, EPOCHS):\n",
    "\t# show the current epoch number\n",
    "\tprint(\"[INFO] starting epoch {}/{}...\".format(\n",
    "\t\tepoch + 1, EPOCHS), end=\"\")\n",
    "\tsys.stdout.flush()\n",
    "\tepochStart = time.time()\n",
    "\n",
    "\t# loop over the data in batch size increments\n",
    "\tfor i in range(0, numUpdates):\n",
    "\t\t# determine starting and ending slice indexes for the current\n",
    "\t\t# batch\n",
    "\t\tstart = i * BS\n",
    "\t\tend = start + BS\n",
    "\n",
    "\t\t# take a step\n",
    "\t\tstep(trainX[start:end], trainY[start:end])\n",
    "\n",
    "\t# show timing information for the epoch\n",
    "\tepochEnd = time.time()\n",
    "\telapsed = (epochEnd - epochStart) / 60.0\n",
    "\tprint(\"took {:.4} minutes\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll calculate the loss and accuracy on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 6s 551us/sample - loss: 0.0407 - acc: 0.9930\n[INFO] test accuracy: 0.9930\n"
    }
   ],
   "source": [
    "# in order to calculate accuracy using Keras' functions we first need\n",
    "# to compile the model\n",
    "model.compile(optimizer=opt, loss=categorical_crossentropy,\n",
    "\tmetrics=[\"acc\"])\n",
    "\n",
    "# now that the model is compiled we can compute the accuracy\n",
    "(loss, acc) = model.evaluate(testX, testY)\n",
    "print(\"[INFO] test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape allows us to track TensorFlow computations and calculate gradients w.r.t. (with respect to) some given variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "27.0\n"
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, GradientTape doesn’t track constants, so we must instruct it to with: tape.watch(variable)\n",
    "\n",
    "Then we can perform some computation on the variables we are watching. The computation can be anything from cubing it, x**3, to passing it through a neural network\n",
    "\n",
    "We calculate gradients of a calculation w.r.t. a variable with tape.gradient(target, sources). Note, tape.gradient returns an EagerTensor that you can convert to ndarray format with .numpy()\n",
    "\n",
    "If at any point, we want to use multiple variables in our calculations, all we need to do is give tape.gradient a list or tuple of those variables. When we optimize Keras models, we pass model.trainable_variables as our variable list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Watching Variables\n",
    "\n",
    "If x were a trainable variable instead of a constant, there would be no need to tell the tape to watch it—GradientTape automatically watches all trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "27.0\n"
    }
   ],
   "source": [
    "x = tf.Variable(3.0, trainable=True)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "watch_accessed_variables=False\n",
    "\n",
    "If we don’t want GradientTape to watch all trainable variables automatically, we can set the tape’s watch_accessed_variables parameter to False:\n",
    "\n",
    "Disabling watch_accessed_variables gives us fine control over what variables we want to watch.\n",
    "\n",
    "If you have a lot of trainable variables and are not optimizing them all at once, You may want to disable watch_accessed_variables to protect yourself from mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    }
   ],
   "source": [
    "x = tf.Variable(3.0, trainable=True)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher-Order Derivatives\n",
    "\n",
    "If you want to compute higher-order derivatives, you can use nested GradientTapes:\n",
    "\n",
    "Higher-order derivatives is generally the only time when you would want to compute gradients inside a GradientTape object. Otherwise, it will slow done computations as the GradientTape is watching every computation done in the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "18.0\n"
    }
   ],
   "source": [
    "x = tf.Variable(3.0, trainable=True)\n",
    "with tf.GradientTape() as tape1:\n",
    "    with tf.GradientTape() as tape2:\n",
    "        y = x ** 3\n",
    "    order_1 = tape2.gradient(y, x)\n",
    "order_2 = tape1.gradient(order_1, x)\n",
    "\n",
    "print(order_2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent=True\n",
    "\n",
    "If we were to run the following:\n",
    "\n",
    "a = tf.Variable(6.0, trainable=True)\n",
    "b = tf.Variable(2.0, trainable=True)\n",
    "with tf.GradientTape() as tape:\n",
    "    y1 = a ** 2\n",
    "    y2 = b ** 3\n",
    "\n",
    "print(tape.gradient(y1, a).numpy())\n",
    "print(tape.gradient(y2, b).numpy())\n",
    "\n",
    "But in reality, calling tape.gradient a second time will raise an error.\n",
    "\n",
    "This is because immediately after calling tape.gradient, the GradientTape releases all the information stored inside of it for computational purposes.\n",
    "\n",
    "If we want to bypass this, we can set persistent=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "12.0\n12.0\n"
    }
   ],
   "source": [
    "a = tf.Variable(6.0, trainable=True)\n",
    "b = tf.Variable(2.0, trainable=True)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1 = a ** 2\n",
    "    y2 = b ** 3\n",
    "print(tape.gradient(y1, a).numpy())\n",
    "print(tape.gradient(y2, b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop_recording()\n",
    "\n",
    "tape.stop_recording() temporarily pauses the tapes recording, leading to greater computation speed.\n",
    "\n",
    "In my opinion, in long functions, it is more readable to use stop_recording blocks multiple times to calculate gradients in the middle of a function, than to calculate all the gradients at the end of a function.\n",
    "\n",
    "For example, I prefer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "12.0\n12.0\n"
    }
   ],
   "source": [
    "a = tf.Variable(6.0, trainable=True)\n",
    "b = tf.Variable(2.0, trainable=True)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1 = a ** 2\n",
    "    with tape.stop_recording():\n",
    "        print(tape.gradient(y1, a).numpy())\n",
    "    \n",
    "    y2 = b ** 3\n",
    "    with tape.stop_recording():\n",
    "        print(tape.gradient(y2, b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Methods\n",
    "\n",
    "Although I won’t go into detail here, GradientTape has a few other handy methods, including:\n",
    "\n",
    "    .jacobian: “Computes the jacobian using operations recorded in context of this tape.”\n",
    "    .batch_jacobian: “Computes and stacks per-example jacobians.”\n",
    "    .reset: “Clears all information stored in this tape.”\n",
    "    .watched_variables: “Returns variables watched by this tape in order of construction.”\n",
    "\n",
    "All above information quoted from the GradientTape documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Uses\n",
    "2.0 — Linear Regression\n",
    "\n",
    "To start off the more advanced uses of GradientTape, let’s look at a classic “Hello World!” to ML: linear regression.\n",
    "\n",
    "First, we start by defining a few essential variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Loss function\n",
    "def loss(real_y, pred_y):\n",
    "    return tf.abs(real_y - pred_y)\n",
    "\n",
    "# Training data\n",
    "x_train = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_train = np.asarray([i*10+5 for i in x_train]) # y = 10x+5\n",
    "\n",
    "# Trainable variables\n",
    "a = tf.Variable(random.random(), trainable=True)\n",
    "b = tf.Variable(random.random(), trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead and define our step function. The step function will be run every epoch to update the trainable variables, a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(real_x, real_y):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Make prediction\n",
    "        pred_y = a * real_x + b\n",
    "        # Calculate loss\n",
    "        reg_loss = loss(real_y, pred_y)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    a_gradients, b_gradients = tape.gradient(reg_loss, (a, b))\n",
    "\n",
    "    # Update variables\n",
    "    a.assign_sub(a_gradients * 0.001)\n",
    "    b.assign_sub(b_gradients * 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y ≈ 9.993399620056152x + 4.990865230560303\n"
    }
   ],
   "source": [
    "for _ in range(100000):\n",
    "    step(x_train, y_train)\n",
    "\n",
    "print(f'y ≈ {a.numpy()}x + {b.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}